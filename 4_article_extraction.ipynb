{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d91ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d8e55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas\n",
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "PATH_ARTICULOS_CITADOS = Path(os.path.join(PATH, \"articulos_citados_hard\"))\n",
    "\n",
    "# Ejecutar extracci√≥n (puedes modificar los patrones regex despu√©s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "687136ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cited_articles_and_laws(json_dir: str, output_dir: str, regex_patterns: list = None):\n",
    "    \"\"\"\n",
    "    Extrae art√≠culos y leyes citados de cada subsecci√≥n de CONTENIDO en los JSONs.\n",
    "    \n",
    "    Args:\n",
    "        json_dir: Directorio con los JSONs originales (ej: datasets/fallos_json)\n",
    "        output_dir: Directorio de salida (ej: datasets/articulos_citados_hard)\n",
    "        regex_patterns: Lista de patrones regex para extraer citas (opcional)\n",
    "    \"\"\"\n",
    "    json_root = Path(json_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Patrones regex mejorados\n",
    "    if regex_patterns is None:\n",
    "        regex_patterns = [\n",
    "            # Art√≠culos con diferentes formatos\n",
    "            r'-?arts?\\.\\s*(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # arts. 3, 14, 29 y 94 o -arts. 1¬∫ y 4¬∫\n",
    "            r'Art(?:√≠culo)?s?\\.\\s*(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # Art. 28, Art√≠culo 45\n",
    "            r'del\\s+art\\.?\\s*(\\d+(?:¬∫|¬∞)?)',  # del art.114\n",
    "            r'art√≠culos?\\s+(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # art√≠culo 123 y 456\n",
    "            \n",
    "            # Leyes con diferentes formatos\n",
    "            r'ley\\s+n?¬∫?\\s*(\\d+(?:/\\d+)?)',  # ley 7046, ley n¬∫ 5678/90\n",
    "            r'leyes?\\s+n?¬∫?\\s*(\\d+(?:/\\d+)?)(?:\\s*[,y]\\s*(\\d+(?:/\\d+)?))*',  # leyes 123 y 456\n",
    "            \n",
    "            # N√∫meros standalone despu√©s de menciones de art√≠culos (para capturar secuencias)\n",
    "            r'(?:arts?\\.|art√≠culos?|Art\\.)\\s*[^\\d]*(\\d+(?:¬∫|¬∞)?(?:\\s*[,y]\\s*\\d+(?:¬∫|¬∞)?)*(?:\\s*y\\s*\\d+(?:¬∫|¬∞)?)?)',\n",
    "        ]\n",
    "    \n",
    "    def extract_numbers_from_match(match_groups):\n",
    "        \"\"\"Extrae todos los n√∫meros de los grupos de una coincidencia regex\"\"\"\n",
    "        numbers = []\n",
    "        for group in match_groups:\n",
    "            if group:  # Si el grupo no es None\n",
    "                # Buscar todos los n√∫meros en el grupo\n",
    "                nums = re.findall(r'\\d+(?:¬∫|¬∞)?', group)\n",
    "                numbers.extend(nums)\n",
    "        return numbers\n",
    "    \n",
    "    def extract_articles_from_text(text):\n",
    "        \"\"\"Extrae art√≠culos de un texto usando m√∫ltiples estrategias\"\"\"\n",
    "        all_articles = set()  # Usar set para evitar duplicados\n",
    "        \n",
    "        # Estrategia 1: Patrones espec√≠ficos\n",
    "        for pattern in compiled_patterns:\n",
    "            matches = pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                numbers = extract_numbers_from_match(match.groups())\n",
    "                all_articles.update(numbers)\n",
    "        \n",
    "        # Estrategia 2: Buscar secuencias espec√≠ficas como \"3, 14, 29, 30, 63, 64, 71 y 94\"\n",
    "        # Patr√≥n para capturar listas de n√∫meros despu√©s de \"arts.\" o similar\n",
    "        sequence_pattern = r'(?:-?arts?\\.|art√≠culos?|Art\\.)\\s*([0-9¬∫¬∞,\\s\\-y]+?)(?:\\s+de\\s+la\\s+ley|\\s+Ac\\.|\\.|\\s|$)'\n",
    "        seq_matches = re.finditer(sequence_pattern, text, re.IGNORECASE)\n",
    "        for match in seq_matches:\n",
    "            sequence = match.group(1)\n",
    "            # Extraer todos los n√∫meros de la secuencia\n",
    "            nums = re.findall(r'\\d+(?:¬∫|¬∞)?', sequence)\n",
    "            all_articles.update(nums)\n",
    "        \n",
    "        return list(all_articles)\n",
    "    \n",
    "    # Compilar patrones\n",
    "    compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in regex_patterns]\n",
    "    \n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Extrayendo citas de {len(json_files)} archivos JSON...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Extrayendo citas\"):\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)[0]  # Primer elemento de la lista\n",
    "            \n",
    "            # Crear estructura de salida\n",
    "            output_structure = {\n",
    "                \"INFORMACION\": data.get(\"INFORMACION\", {}),\n",
    "                \"CONTENIDO\": {}\n",
    "            }\n",
    "            \n",
    "            # Procesar cada subsecci√≥n de CONTENIDO\n",
    "            if 'CONTENIDO' in data:\n",
    "                for section_name, paragraphs in data['CONTENIDO'].items():\n",
    "                    cited_articles = []\n",
    "                    \n",
    "                    # Buscar citas en cada p√°rrafo de la subsecci√≥n\n",
    "                    for paragraph in paragraphs:\n",
    "                        if isinstance(paragraph, str):\n",
    "                            articles = extract_articles_from_text(paragraph)\n",
    "                            cited_articles.extend(articles)\n",
    "                    \n",
    "                    # Remover duplicados y ordenar\n",
    "                    cited_articles = sorted(list(set(cited_articles)))\n",
    "                    \n",
    "                    # Guardar lista de citas para esta subsecci√≥n\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = cited_articles\n",
    "            \n",
    "            # Crear archivo de salida manteniendo estructura de carpetas\n",
    "            rel_path = json_path.relative_to(json_root)\n",
    "            output_path = output_root / rel_path\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Guardar JSON con citas extra√≠das\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando {json_path}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Extracci√≥n completada. Archivos guardados en: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1751bef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extrayendo citas de 296 archivos JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo citas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [00:00<00:00, 915.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracci√≥n completada. Archivos guardados en: /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/articulos_citados_hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_cited_articles_and_laws(PATH_JSON, PATH_ARTICULOS_CITADOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AzureOpenAI\n",
    "from configs.credentials_config import API_KEY, ENDPOINT, MODEL, DEPLOYMENT\n",
    "\n",
    "# Pydantic schemas\n",
    "class CitedLaw(BaseModel):\n",
    "    \"\"\"Schema for a cited law with its articles\"\"\"\n",
    "    number: str = Field(description=\"Law number (e.g., '7046', '123/90')\")\n",
    "    articles: List[str] = Field(description=\"List of article numbers cited from this law\", default_factory=list)\n",
    "\n",
    "class SubsectionCitations(BaseModel):\n",
    "    \"\"\"Schema for citations found in a subsection\"\"\"\n",
    "    cited_laws: List[CitedLaw] = Field(description=\"List of laws cited with their articles\", default_factory=list)\n",
    "    standalone_articles: List[str] = Field(description=\"Articles mentioned without specific law reference\", default_factory=list)\n",
    "\n",
    "class ContentCitations(BaseModel):\n",
    "    \"\"\"Schema for all citations in CONTENIDO sections\"\"\"\n",
    "    sections: dict[str, SubsectionCitations] = Field(description=\"Citations organized by section name\", default_factory=dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18db274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=ENDPOINT,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "def extract_citations_with_llm(text: str, section_name: str) -> SubsectionCitations:\n",
    "    \"\"\"\n",
    "    Extract legal citations from text using LLM with structured output\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are a legal text analyzer. Extract all legal citations from the following text from the \"{section_name}\" section of a legal document.\n",
    "\n",
    "Find and structure:\n",
    "1. Laws with their specific articles (e.g., \"ley 7046\" with \"arts. 3, 14, 29\")\n",
    "2. Articles mentioned without specific law reference (e.g., \"Art. 28\", \"del art.114\")\n",
    "\n",
    "Text to analyze:\n",
    "{text}\n",
    "\n",
    "Extract ALL article and law numbers mentioned. Include ordinal numbers (1¬∫, 4¬∫) and regular numbers.\n",
    "Be thorough and don't miss any citations, especially in sequences like \"arts. 3, 14, 29, 30, 63, 64, 71 y 94\".\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.beta.chat.completions.parse(\n",
    "            model=DEPLOYMENT, #modify this to use responses api, as in Doc's work\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a precise legal citation extractor. Extract all article and law numbers mentioned in legal texts. Be thorough and accurate.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            response_format=SubsectionCitations,\n",
    "            temperature=0.1  # Low temperature for consistency\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.parsed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {section_name}: {e}\")\n",
    "        return SubsectionCitations()\n",
    "\n",
    "def process_json_with_llm(json_path: Path, output_path: Path):\n",
    "    \"\"\"\n",
    "    Process a single JSON file and extract citations using LLM\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)[0]  # First element of the list\n",
    "        \n",
    "        # Create output structure\n",
    "        output_structure = {\n",
    "            \"INFORMACION\": data.get(\"INFORMACION\", {}),\n",
    "            \"CONTENIDO\": {}\n",
    "        }\n",
    "        \n",
    "        # Process each subsection of CONTENIDO\n",
    "        if 'CONTENIDO' in data:\n",
    "            for section_name, paragraphs in data['CONTENIDO'].items():\n",
    "                if paragraphs:  # Only process non-empty sections\n",
    "                    # Join paragraphs into single text for analysis\n",
    "                    section_text = \"\\n\\n\".join(paragraphs) if isinstance(paragraphs, list) else str(paragraphs)\n",
    "                    \n",
    "                    # Extract citations using LLM\n",
    "                    citations = extract_citations_with_llm(section_text, section_name)\n",
    "                    \n",
    "                    # Convert to dict format for JSON serialization\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = citations.model_dump()\n",
    "                else:\n",
    "                    # Empty section\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = SubsectionCitations().model_dump()\n",
    "        \n",
    "        # Save results\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {json_path}: {e}\")\n",
    "\n",
    "def extract_citations_with_llm_batch(json_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process all JSON files and extract citations using LLM\n",
    "    \"\"\"\n",
    "    json_root = Path(json_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ü§ñ Extrayendo citas con LLM de {len(json_files)} archivos JSON...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Procesando con LLM\"):\n",
    "        # Maintain folder structure\n",
    "        rel_path = json_path.relative_to(json_root)\n",
    "        output_path = output_root / rel_path\n",
    "        \n",
    "        process_json_with_llm(json_path, output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Extracci√≥n con LLM completada. Archivos guardados en: {output_root}\")\n",
    "\n",
    "# Define paths\n",
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "PATH_ARTICULOS_LLM = Path(os.path.join(PATH, \"articulos_citados_llm\"))\n",
    "\n",
    "# Execute LLM extraction\n",
    "extract_citations_with_llm_batch(PATH_JSON, PATH_ARTICULOS_LLM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
