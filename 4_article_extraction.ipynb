{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d91ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d8e55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir rutas\n",
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "PATH_ARTICULOS_CITADOS = Path(os.path.join(PATH, \"articulos_citados_hard\"))\n",
    "\n",
    "# Ejecutar extracci√≥n (puedes modificar los patrones regex despu√©s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687136ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cited_articles_and_laws(json_dir: str, output_dir: str, regex_patterns: list = None):\n",
    "    \"\"\"\n",
    "    Extrae art√≠culos y leyes citados de cada subsecci√≥n de CONTENIDO en los JSONs.\n",
    "    \n",
    "    Args:\n",
    "        json_dir: Directorio con los JSONs originales (ej: datasets/fallos_json)\n",
    "        output_dir: Directorio de salida (ej: datasets/articulos_citados_hard)\n",
    "        regex_patterns: Lista de patrones regex para extraer citas (opcional)\n",
    "    \"\"\"\n",
    "    json_root = Path(json_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Patrones regex mejorados\n",
    "    if regex_patterns is None:\n",
    "        regex_patterns = [\n",
    "            # Art√≠culos con diferentes formatos\n",
    "            r'-?arts?\\.\\s*(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # arts. 3, 14, 29 y 94 o -arts. 1¬∫ y 4¬∫\n",
    "            r'Art(?:√≠culo)?s?\\.\\s*(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # Art. 28, Art√≠culo 45\n",
    "            r'del\\s+art\\.?\\s*(\\d+(?:¬∫|¬∞)?)',  # del art.114\n",
    "            r'art√≠culos?\\s+(\\d+(?:¬∫|¬∞)?)(?:\\s*[,y]\\s*(\\d+(?:¬∫|¬∞)?))*',  # art√≠culo 123 y 456\n",
    "            \n",
    "            # Leyes con diferentes formatos\n",
    "            r'ley\\s+n?¬∫?\\s*(\\d+(?:/\\d+)?)',  # ley 7046, ley n¬∫ 5678/90\n",
    "            r'leyes?\\s+n?¬∫?\\s*(\\d+(?:/\\d+)?)(?:\\s*[,y]\\s*(\\d+(?:/\\d+)?))*',  # leyes 123 y 456\n",
    "            \n",
    "            # N√∫meros standalone despu√©s de menciones de art√≠culos (para capturar secuencias)\n",
    "            r'(?:arts?\\.|art√≠culos?|Art\\.)\\s*[^\\d]*(\\d+(?:¬∫|¬∞)?(?:\\s*[,y]\\s*\\d+(?:¬∫|¬∞)?)*(?:\\s*y\\s*\\d+(?:¬∫|¬∞)?)?)',\n",
    "        ]\n",
    "    \n",
    "    def extract_numbers_from_match(match_groups):\n",
    "        \"\"\"Extrae todos los n√∫meros de los grupos de una coincidencia regex\"\"\"\n",
    "        numbers = []\n",
    "        for group in match_groups:\n",
    "            if group:  # Si el grupo no es None\n",
    "                # Buscar todos los n√∫meros en el grupo\n",
    "                nums = re.findall(r'\\d+(?:¬∫|¬∞)?', group)\n",
    "                numbers.extend(nums)\n",
    "        return numbers\n",
    "    \n",
    "    def extract_articles_from_text(text):\n",
    "        \"\"\"Extrae art√≠culos de un texto usando m√∫ltiples estrategias\"\"\"\n",
    "        all_articles = set()  # Usar set para evitar duplicados\n",
    "        \n",
    "        # Estrategia 1: Patrones espec√≠ficos\n",
    "        for pattern in compiled_patterns:\n",
    "            matches = pattern.finditer(text)\n",
    "            for match in matches:\n",
    "                numbers = extract_numbers_from_match(match.groups())\n",
    "                all_articles.update(numbers)\n",
    "        \n",
    "        # Estrategia 2: Buscar secuencias espec√≠ficas como \"3, 14, 29, 30, 63, 64, 71 y 94\"\n",
    "        # Patr√≥n para capturar listas de n√∫meros despu√©s de \"arts.\" o similar\n",
    "        sequence_pattern = r'(?:-?arts?\\.|art√≠culos?|Art\\.)\\s*([0-9¬∫¬∞,\\s\\-y]+?)(?:\\s+de\\s+la\\s+ley|\\s+Ac\\.|\\.|\\s|$)'\n",
    "        seq_matches = re.finditer(sequence_pattern, text, re.IGNORECASE)\n",
    "        for match in seq_matches:\n",
    "            sequence = match.group(1)\n",
    "            # Extraer todos los n√∫meros de la secuencia\n",
    "            nums = re.findall(r'\\d+(?:¬∫|¬∞)?', sequence)\n",
    "            all_articles.update(nums)\n",
    "        \n",
    "        return list(all_articles)\n",
    "    \n",
    "    # Compilar patrones\n",
    "    compiled_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in regex_patterns]\n",
    "    \n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Extrayendo citas de {len(json_files)} archivos JSON...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Extrayendo citas\"):\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)[0]  # Primer elemento de la lista\n",
    "            \n",
    "            # Crear estructura de salida\n",
    "            output_structure = {\n",
    "                \"INFORMACION\": data.get(\"INFORMACION\", {}),\n",
    "                \"CONTENIDO\": {}\n",
    "            }\n",
    "            \n",
    "            # Procesar cada subsecci√≥n de CONTENIDO\n",
    "            if 'CONTENIDO' in data:\n",
    "                for section_name, paragraphs in data['CONTENIDO'].items():\n",
    "                    cited_articles = []\n",
    "                    \n",
    "                    # Buscar citas en cada p√°rrafo de la subsecci√≥n\n",
    "                    for paragraph in paragraphs:\n",
    "                        if isinstance(paragraph, str):\n",
    "                            articles = extract_articles_from_text(paragraph)\n",
    "                            cited_articles.extend(articles)\n",
    "                    \n",
    "                    # Remover duplicados y ordenar\n",
    "                    cited_articles = sorted(list(set(cited_articles)))\n",
    "                    \n",
    "                    # Guardar lista de citas para esta subsecci√≥n\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = cited_articles\n",
    "            \n",
    "            # Crear archivo de salida manteniendo estructura de carpetas\n",
    "            rel_path = json_path.relative_to(json_root)\n",
    "            output_path = output_root / rel_path\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Guardar JSON con citas extra√≠das\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error procesando {json_path}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Extracci√≥n completada. Archivos guardados en: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1751bef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extrayendo citas de 296 archivos JSON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extrayendo citas: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [00:00<00:00, 763.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error procesando /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/fallos_json/10/9093.json: Expecting value: line 73 column 5 (char 16919)\n",
      "‚úÖ Extracci√≥n completada. Archivos guardados en: /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/articulos_citados_hard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_cited_articles_and_laws(PATH_JSON, PATH_ARTICULOS_CITADOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f13f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import AzureOpenAI\n",
    "from configs.credentials_config import API_KEY, ENDPOINT, MODEL, DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b161cff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsectionCitations(BaseModel):\n",
    "    \"\"\"Schema for citations found in a subsection\"\"\"\n",
    "    verbatim_citations: List[str] = Field(description=\"List of exact citation phrases as they appear in the text (e.g., 'arts. 1¬∫ y 4¬∫ Ac. Gral. 15/18 SNE', 'arts. 3, 29, 30, 63, 64, 70 y 94 de la ley 7046')\", default_factory=list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18db274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2025-04-01-preview\",\n",
    "    azure_endpoint=ENDPOINT,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "def extract_citations_with_llm(text: str, section_name: str) -> SubsectionCitations:\n",
    "    \"\"\"\n",
    "    Extract legal citations from text using LLM with structured output\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a legal text analyzer. Extract all legal citations as complete verbatim phrases from the following text from the \"{section_name}\" section of a legal document.\n",
    "\n",
    "    IMPORTANT: Extract the COMPLETE citation phrases exactly as they appear in the text, including:\n",
    "    - Article numbers with their source (e.g., \"art. 45 del CPCC\", \"arts. 1¬∫ y 4¬∫ Ac. Gral. 15/18 SNE\")\n",
    "    - Laws with articles (e.g., \"arts. 3, 29, 30, 63, 64, 70 y 94 de la ley 7046\")\n",
    "    - Constitutional articles (e.g., \"art. 14 de la Constituci√≥n Nacional\")\n",
    "    - Code articles (e.g., \"art. 163 del C√≥digo Civil\", \"art. 280 del CPCC\")\n",
    "    - Procedural references (e.g., \"conforme arts. 1¬∫ y 4¬∫ Ac. Gral. 15/18 SNE\")\n",
    "    - Any legal reference with acronyms (CPCC, CN, CC, etc.)\n",
    "\n",
    "    Text to analyze:\n",
    "    {text}\n",
    "\n",
    "    Return ONLY the complete verbatim phrases as they appear in the text. Do NOT break them down or parse them - capture the entire citation phrase including the source/acronym when present.\n",
    "\n",
    "    Examples of what to extract:\n",
    "    - \"arts. 1¬∫ y 4¬∫ Ac. Gral. 15/18 SNE\"\n",
    "    - \"arts. 3, 29, 30, 63, 64, 70 y 94 de la ley 7046\"\n",
    "    - \"del art. 114\"\n",
    "    - \"art. 45 del CPCC\"\n",
    "    - \"Art. 28\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=DEPLOYMENT, #modify this to use responses api, as in Doc's work\n",
    "            instructions = \"You are a precise legal citation extractor. Extract all article and law numbers mentioned in legal texts. Be thorough and accurate.\",\n",
    "            input        = prompt,\n",
    "            text_format=SubsectionCitations,\n",
    "            temperature=0.1  # Low temperature for consistency\n",
    "        )\n",
    "        \n",
    "        return response.output_parsed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing section {section_name}: {e}\")\n",
    "        return SubsectionCitations()\n",
    "\n",
    "def process_json_with_llm(json_path: Path, output_path: Path):\n",
    "    \"\"\"\n",
    "    Process a single JSON file and extract citations using LLM\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)[0]  # First element of the list\n",
    "        \n",
    "        # Create output structure\n",
    "        output_structure = {\n",
    "            \"INFORMACION\": data.get(\"INFORMACION\", {}),\n",
    "            \"CONTENIDO\": {}\n",
    "        }\n",
    "        \n",
    "        # Process each subsection of CONTENIDO\n",
    "        if 'CONTENIDO' in data:\n",
    "            for section_name, paragraphs in data['CONTENIDO'].items():\n",
    "                if paragraphs:  # Only process non-empty sections\n",
    "                    # Join paragraphs into single text for analysis\n",
    "                    section_text = \"\\n\\n\".join(paragraphs) if isinstance(paragraphs, list) else str(paragraphs)\n",
    "                    \n",
    "                    # Extract citations using LLM\n",
    "                    citations = extract_citations_with_llm(section_text, section_name)\n",
    "                    \n",
    "                    # Convert to dict format for JSON serialization\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = citations.model_dump()\n",
    "                    print(citations.model_dump())\n",
    "                else:\n",
    "                    # Empty section\n",
    "                    output_structure[\"CONTENIDO\"][section_name] = SubsectionCitations().model_dump()\n",
    "        \n",
    "        # Save results\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {json_path}: {e}\")\n",
    "\n",
    "def extract_citations_with_llm_batch(json_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process all JSON files and extract citations using LLM\n",
    "    \"\"\"\n",
    "    json_root = Path(json_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ü§ñ Extrayendo citas con LLM de {len(json_files)} archivos JSON...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Procesando con LLM\"):\n",
    "        # Maintain folder structure\n",
    "        rel_path = json_path.relative_to(json_root)\n",
    "        output_path = output_root / rel_path\n",
    "        \n",
    "        process_json_with_llm(json_path, output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Extracci√≥n con LLM completada. Archivos guardados en: {output_root}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d373793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "PATH_ARTICULOS_LLM = Path(os.path.join(PATH, \"articulos_extraidos_llm\"))\n",
    "\n",
    "\n",
    "# Execute LLM extraction\n",
    "extract_citations_with_llm_batch(PATH_JSON, PATH_ARTICULOS_LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbe9762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunocr/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RulingCitation(BaseModel):\n",
    "    \"\"\"Schema for a single citation source in a ruling\"\"\"\n",
    "    main_source: str = Field(description=\"Main legal source (e.g., 'ley 7046', 'Constituci√≥n Nacional', 'C√≥digo Civil', 'CPCC', 'Ac. Gral. 15/18 SNE')\")\n",
    "    cited_articles: Optional[List[int]] = Field(description=\"List of article numbers cited from this source\", default=None)\n",
    "    extra: Optional[str] = Field(description=\"Additional information that couldn't be captured in source or articles (e.g., ordinal indicators like '1¬∫', '4¬∫', specific sections)\", default=None)\n",
    "\n",
    "class RulingCitations(BaseModel):\n",
    "    \"\"\"Schema for all citations in an entire ruling\"\"\"\n",
    "    citations: List[RulingCitation] = Field(description=\"List of all legal sources cited in the ruling\", default_factory=list)\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"citations\": [\n",
    "                    {\n",
    "                        \"main_source\": \"ley 7046\",\n",
    "                        \"cited_articles\": [3, 29, 30, 63, 64, 70, 94],\n",
    "                        \"extra\": None\n",
    "                    },\n",
    "                    {\n",
    "                        \"main_source\": \"Ac. Gral. 15/18 SNE\",\n",
    "                        \"cited_articles\": [1, 4],\n",
    "                        \"extra\": \"1¬∫ y 4¬∫\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"main_source\": \"CPCC\",\n",
    "                        \"cited_articles\": [114],\n",
    "                        \"extra\": None\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "221dd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ruling_citations_with_llm(verbatim_citations_list: List[str]) -> RulingCitations:\n",
    "    \"\"\"\n",
    "    Aggregate all verbatim citations from a ruling into structured RulingCitations\n",
    "    \"\"\"\n",
    "    # Join all citations for analysis\n",
    "    all_citations_text = \"\\n\".join(verbatim_citations_list)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a legal citation analyzer. Take these verbatim legal citations from a judicial ruling and organize them into structured legal sources with their cited articles.\n",
    "\n",
    "    Verbatim citations to analyze:\n",
    "    {all_citations_text}\n",
    "\n",
    "    Your task:\n",
    "    1. Identify the main legal sources (laws, codes, constitutions, agreements, etc.)\n",
    "    2. Group article numbers by their source\n",
    "    3. Capture any additional information that doesn't fit in source/articles\n",
    "\n",
    "    Examples of how to structure:\n",
    "    - \"arts. 3, 29, 30, 63, 64, 70 y 94 de la ley 7046\" ‚Üí main_source: \"ley 7046\", cited_articles: [3, 29, 30, 63, 64, 70, 94]\n",
    "    - \"arts. 1¬∫ y 4¬∫ Ac. Gral. 15/18 SNE\" ‚Üí main_source: \"Ac. Gral. 15/18 SNE\", cited_articles: [1, 4], extra: \"1¬∫ y 4¬∫\"\n",
    "    - \"art. 45 del CPCC\" ‚Üí main_source: \"CPCC\", cited_articles: [45]\n",
    "    - \"del art. 114\" ‚Üí main_source: \"unknown\", cited_articles: [114]\n",
    "\n",
    "\n",
    "    Some of the strings may only contain article numbers without a clear source, like \"arts. 1¬∫ y 4¬∫\". In these cases, ignore the orphan articles altogether.\n",
    "    Combine duplicates and organize by source.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=DEPLOYMENT,\n",
    "            instructions=\"You are a legal citation organizer. Structure verbatim citations into organized legal sources with their articles.\",\n",
    "            input=prompt,\n",
    "            text_format=RulingCitations,\n",
    "            temperature=0.01\n",
    "        )\n",
    "        \n",
    "        return response.output_parsed\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error aggregating ruling citations: {e}\")\n",
    "        return RulingCitations()\n",
    "\n",
    "def process_verbatim_to_structured(input_json_path: Path, output_path: Path):\n",
    "    \"\"\"\n",
    "    Process a JSON with verbatim citations and create structured ruling citations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)[0]  # First element of the list\n",
    "        \n",
    "        # Collect all verbatim citations from all sections\n",
    "        all_verbatim_citations = []\n",
    "        \n",
    "        if 'CONTENIDO' in data:\n",
    "            for section_name, section_data in data['CONTENIDO'].items():\n",
    "                if 'verbatim_citations' in section_data:\n",
    "                    all_verbatim_citations.extend(section_data['verbatim_citations'])\n",
    "        \n",
    "        # Skip if no citations found\n",
    "        if not all_verbatim_citations:\n",
    "            print(f\"‚ö†Ô∏è  No citations found in {input_json_path.name}\")\n",
    "            return\n",
    "        \n",
    "        # Aggregate citations using LLM\n",
    "        structured_citations = aggregate_ruling_citations_with_llm(all_verbatim_citations)\n",
    "        \n",
    "        # Create output structure\n",
    "        output_structure = {\n",
    "            \"INFORMACION\": data.get(\"INFORMACION\", {}),\n",
    "            \"RULING_CITATIONS\": structured_citations.model_dump(),\n",
    "            \"METADATA\": {\n",
    "                \"total_verbatim_citations\": len(all_verbatim_citations),\n",
    "                \"total_structured_sources\": len(structured_citations.citations),\n",
    "                \"original_citations\": all_verbatim_citations\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {input_json_path}: {e}\")\n",
    "\n",
    "def create_structured_citations_batch(input_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process all verbatim citation JSONs and create structured ruling citations\n",
    "    \"\"\"\n",
    "    input_root = Path(input_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    json_files = list(input_root.rglob(\"*.json\"))\n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîÑ Estructurando citas de {len(json_files)} fallos judiciales...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Estructurando citas\"):\n",
    "        # Maintain folder structure\n",
    "        rel_path = json_path.relative_to(input_root)\n",
    "        output_path = output_root / rel_path\n",
    "        \n",
    "        process_verbatim_to_structured(json_path, output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Estructuraci√≥n completada. Archivos guardados en: {output_root}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c696d880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Estructurando citas de 295 fallos judiciales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estructurando citas:  33%|‚ñà‚ñà‚ñà‚ñé      | 96/295 [04:26<09:12,  2.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m PATH_ARTICULOS_ESTRUCTURADOS = Path(os.path.join(PATH, \u001b[33m\"\u001b[39m\u001b[33marticulos_estructurados\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Execute structured citation creation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mcreate_structured_citations_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_ARTICULOS_LLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_ARTICULOS_ESTRUCTURADOS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mcreate_structured_citations_batch\u001b[39m\u001b[34m(input_dir, output_dir)\u001b[39m\n\u001b[32m    105\u001b[39m     rel_path = json_path.relative_to(input_root)\n\u001b[32m    106\u001b[39m     output_path = output_root / rel_path\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mprocess_verbatim_to_structured\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Estructuraci√≥n completada. Archivos guardados en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mprocess_verbatim_to_structured\u001b[39m\u001b[34m(input_json_path, output_path)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Aggregate citations using LLM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m structured_citations = \u001b[43maggregate_ruling_citations_with_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_verbatim_citations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Create output structure\u001b[39;00m\n\u001b[32m     70\u001b[39m output_structure = {\n\u001b[32m     71\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mINFORMACION\u001b[39m\u001b[33m\"\u001b[39m: data.get(\u001b[33m\"\u001b[39m\u001b[33mINFORMACION\u001b[39m\u001b[33m\"\u001b[39m, {}),\n\u001b[32m     72\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRULING_CITATIONS\u001b[39m\u001b[33m\"\u001b[39m: structured_citations.model_dump(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     }\n\u001b[32m     78\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36maggregate_ruling_citations_with_llm\u001b[39m\u001b[34m(verbatim_citations_list)\u001b[39m\n\u001b[32m      8\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33mYou are a legal citation analyzer. Take these verbatim legal citations from a judicial ruling and organize them into structured legal sources with their cited articles.\u001b[39m\n\u001b[32m     10\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33mCombine duplicates and organize by source.\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEPLOYMENT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mYou are a legal citation organizer. Structure verbatim citations into organized legal sources with their articles.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRulingCitations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.output_parsed\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/openai/resources/responses/responses.py:936\u001b[39m, in \u001b[36mResponses.parse\u001b[39m\u001b[34m(self, input, model, text_format, tools, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, store, stream, temperature, text, tool_choice, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_response: Response) -> ParsedResponse[TextFormatT]:\n\u001b[32m    930\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(\n\u001b[32m    931\u001b[39m         input_tools=tools,\n\u001b[32m    932\u001b[39m         text_format=text_format,\n\u001b[32m    933\u001b[39m         response=raw_response,\n\u001b[32m    934\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m936\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `Response` instance into a `ParsedResponse`\u001b[39;49;00m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedResponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTextFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/openai/_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UDESA/NLP/TP_NLP/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define paths and execute\n",
    "PATH_ARTICULOS_ESTRUCTURADOS = Path(os.path.join(PATH, \"articulos_estructurados\"))\n",
    "\n",
    "# Execute structured citation creation\n",
    "create_structured_citations_batch(PATH_ARTICULOS_LLM, PATH_ARTICULOS_ESTRUCTURADOS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
