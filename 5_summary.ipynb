{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49b253f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import AzureOpenAI\n",
    "from configs.credentials_config import API_KEY, ENDPOINT, MODEL, DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c187c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "PATH_SUMMARIES = Path(os.path.join(PATH, \"summaries\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f52b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2025-04-01-preview\",\n",
    "    azure_endpoint=ENDPOINT,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "def summarize_judicial_ruling(full_text: str, case_info: dict) -> str:\n",
    "    \"\"\"\n",
    "    Generate core summary of entire judicial ruling using Azure OpenAI\n",
    "    \"\"\"\n",
    "    # Extract case info for context\n",
    "    case_context = \"\"\n",
    "    if case_info:\n",
    "        case_context = f\"Informaci√≥n del caso: {json.dumps(case_info, ensure_ascii=False, indent=2)}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Eres un experto en derecho que analiza fallos judiciales. \n",
    "    Analiza el siguiente fallo judicial completo y extrae la IDEA CENTRAL del fallo.\n",
    "\n",
    "    {case_context}\n",
    "\n",
    "    Instrucciones:\n",
    "    - Identifica la decisi√≥n judicial principal y su fundamento\n",
    "    - Incluye los aspectos legales m√°s relevantes del caso\n",
    "    - Menciona las leyes y art√≠culos clave que sustentan la decisi√≥n\n",
    "    - Explica brevemente el razonamiento del tribunal\n",
    "    - Mant√©n un lenguaje jur√≠dico preciso pero comprensible\n",
    "    - El resumen debe ser conciso pero completo (m√°ximo 300 palabras)\n",
    "\n",
    "    Contenido completo del fallo:\n",
    "    {full_text}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.responses.create(\n",
    "            model=DEPLOYMENT,\n",
    "            instructions=\"Eres un experto jurista que extrae la esencia de fallos judiciales, identificando la decisi√≥n principal y su fundamento legal.\",\n",
    "            input=prompt,\n",
    "            temperature=0.2  # Lower temperature for more consistent legal analysis\n",
    "        )\n",
    "        \n",
    "        return response.output_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating core summary: {e}\")\n",
    "        return \"Error al generar el resumen central del fallo judicial\"\n",
    "\n",
    "def process_json_for_core_summary(json_path: Path, output_path: Path):\n",
    "    \"\"\"\n",
    "    Process a single JSON file and create a core summary of the entire ruling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)[0]  # First element of the list\n",
    "        \n",
    "        # Gather all content from all sections\n",
    "        full_content = []\n",
    "        \n",
    "        if 'CONTENIDO' in data:\n",
    "            for section_name, paragraphs in data['CONTENIDO'].items():\n",
    "                if paragraphs:  # Only process non-empty sections\n",
    "                    section_header = f\"\\n--- {section_name} ---\\n\"\n",
    "                    full_content.append(section_header)\n",
    "                    \n",
    "                    if isinstance(paragraphs, list):\n",
    "                        full_content.extend(paragraphs)\n",
    "                    else:\n",
    "                        full_content.append(str(paragraphs))\n",
    "        \n",
    "        # Join all content into single text\n",
    "        full_text = \"\\n\\n\".join(full_content)\n",
    "        \n",
    "        # Skip if no content\n",
    "        if len(full_text.strip()) < 100:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {json_path.name} - insufficient content\")\n",
    "            return\n",
    "        \n",
    "        # Generate core summary\n",
    "        case_info = data.get(\"INFORMACION\", {})\n",
    "        core_summary = summarize_judicial_ruling(full_text, case_info)\n",
    "        \n",
    "        # Create simplified output structure\n",
    "        output_structure = {\n",
    "            \"INFORMACION\": case_info,\n",
    "            \"CORE_SUMMARY\": core_summary,\n",
    "            \"METADATA\": {\n",
    "                \"longitud_documento_original\": len(full_text),\n",
    "                \"longitud_resumen_central\": len(core_summary),\n",
    "                \"reduccion_porcentual\": round((1 - len(core_summary)/len(full_text)) * 100, 2),\n",
    "                \"secciones_analizadas\": list(data.get('CONTENIDO', {}).keys()) if 'CONTENIDO' in data else []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save results\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump([output_structure], f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {json_path}: {e}\")\n",
    "\n",
    "def generate_core_summaries_batch(json_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Process all JSON files and create core summaries for each judicial ruling\n",
    "    \"\"\"\n",
    "    json_root = Path(json_dir).resolve()\n",
    "    output_root = Path(output_dir).resolve()\n",
    "    output_root.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Debug: Check what folders exist\n",
    "    print(f\"üìÅ Folders found in {json_root}:\")\n",
    "    for folder in sorted(json_root.iterdir()):\n",
    "        if folder.is_dir():\n",
    "            json_count = len(list(folder.glob(\"*.json\")))\n",
    "            print(f\"  {folder.name}: {json_count} JSON files\")\n",
    "    \n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    \n",
    "    # Debug: Sort files to process in order\n",
    "    json_files = sorted(json_files)\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Debug: Show first few files to be processed\n",
    "    print(f\"\\nüìã First 5 files to process:\")\n",
    "    for i, file in enumerate(json_files[:5]):\n",
    "        print(f\"  {i+1}. {file.relative_to(json_root)}\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è  Generando res√∫menes centrales de {len(json_files)} fallos judiciales...\")\n",
    "    \n",
    "    for json_path in tqdm(json_files, desc=\"Analizando fallos\"):\n",
    "        # Maintain folder structure\n",
    "        rel_path = json_path.relative_to(json_root)\n",
    "        output_path = output_root / rel_path\n",
    "        \n",
    "        # Debug: Show which file is being processed\n",
    "        if \"02/\" in str(rel_path):\n",
    "            print(f\"üîç Processing 02 folder file: {rel_path}\")\n",
    "        \n",
    "        process_json_for_core_summary(json_path, output_path)\n",
    "    \n",
    "    print(f\"‚úÖ Res√∫menes centrales completados. Archivos guardados en: {output_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42556063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Folders found in /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/fallos_json:\n",
      "  02: 10 JSON files\n",
      "  03: 30 JSON files\n",
      "  04: 9 JSON files\n",
      "  05: 34 JSON files\n",
      "  06: 45 JSON files\n",
      "  07: 12 JSON files\n",
      "  08: 36 JSON files\n",
      "  09: 19 JSON files\n",
      "  10: 40 JSON files\n",
      "  11: 30 JSON files\n",
      "  12: 31 JSON files\n",
      "\n",
      "üìã First 5 files to process:\n",
      "  1. 02/8104.json\n",
      "  2. 02/8142.json\n",
      "  3. 02/8344.json\n",
      "  4. 02/8569.json\n",
      "  5. 02/8752.json\n",
      "\n",
      "‚öñÔ∏è  Generando res√∫menes centrales de 296 fallos judiciales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   0%|          | 0/296 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8104.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   0%|          | 1/296 [00:04<24:08,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8142.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   1%|          | 2/296 [00:08<20:13,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8344.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   1%|          | 3/296 [00:14<23:58,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8569.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   1%|‚ñè         | 4/296 [00:18<22:24,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8752.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   2%|‚ñè         | 5/296 [00:22<20:49,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8865.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   2%|‚ñè         | 6/296 [00:25<19:53,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8926.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   2%|‚ñè         | 7/296 [00:31<22:24,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8927.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   3%|‚ñé         | 8/296 [00:35<20:57,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8948.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:   3%|‚ñé         | 9/296 [00:40<21:36,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 02 folder file: 02/8971.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 208/296 [15:15<08:08,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error processing /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/fallos_json/10/9093.json: Expecting value: line 73 column 5 (char 16919)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analizando fallos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 296/296 [21:45<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Res√∫menes centrales completados. Archivos guardados en: /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/summaries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_core_summaries_batch(PATH_JSON, PATH_SUMMARIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "183bf385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_single_summary(relative_file_path: str):\n",
    "    \"\"\"\n",
    "    Regenerate summary for a specific file\n",
    "    \n",
    "    Args:\n",
    "        relative_file_path: Path relative to the json root, e.g., \"03/5024.json\"\n",
    "    \"\"\"\n",
    "    json_root = Path(PATH_JSON).resolve()\n",
    "    output_root = Path(PATH_SUMMARIES).resolve()\n",
    "    \n",
    "    # Construct full paths\n",
    "    json_path = json_root / relative_file_path\n",
    "    output_path = output_root / relative_file_path\n",
    "    \n",
    "    # Check if source file exists\n",
    "    if not json_path.exists():\n",
    "        print(f\"‚ùå Source file not found: {json_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîÑ Regenerating summary for: {relative_file_path}\")\n",
    "    \n",
    "    # Process the file\n",
    "    process_json_for_core_summary(json_path, output_path)\n",
    "    \n",
    "    if output_path.exists():\n",
    "        print(f\"‚úÖ Summary regenerated successfully: {output_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed to regenerate summary for: {relative_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d784adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Regenerating summary for: 12/9180.json\n",
      "‚úÖ Summary regenerated successfully: /Users/brunocr/Documents/UDESA/NLP/TP_NLP/datasets/summaries/12/9180.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage:\n",
    "regenerate_single_summary(\"12/9180.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
