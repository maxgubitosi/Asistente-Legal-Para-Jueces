{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b2d68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_20140\\1521052289.py\", line 5, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\__init__.py\", line 14, in <module>\n",
      "    from sentence_transformers.cross_encoder import (\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py\", line 3, in <module>\n",
      "    from .CrossEncoder import CrossEncoder\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py\", line 31, in <module>\n",
      "    from sentence_transformers.cross_encoder.fit_mixin import FitMixin\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\fit_mixin.py\", line 19, in <module>\n",
      "    from sentence_transformers.datasets.NoDuplicatesDataLoader import NoDuplicatesDataLoader\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py\", line 13, in <module>\n",
      "    from .ParallelSentencesDataset import ParallelSentencesDataset\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py\", line 19, in <module>\n",
      "    from sentence_transformers import SentenceTransformer\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 34, in <module>\n",
      "    from sentence_transformers.model_card import SentenceTransformerModelCardData, generate_model_card\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\sentence_transformers\\model_card.py\", line 36, in <module>\n",
      "    from datasets import Dataset, DatasetDict, IterableDataset, IterableDatasetDict, Value\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\__init__.py\", line 43, in <module>\n",
      "    from .arrow_dataset import Dataset\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 65, in <module>\n",
      "    from .arrow_reader import ArrowReader\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 30, in <module>\n",
      "    from .download.download_config import DownloadConfig\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\download\\__init__.py\", line 10, in <module>\n",
      "    from .streaming_download_manager import StreamingDownloadManager\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\download\\streaming_download_manager.py\", line 21, in <module>\n",
      "    from ..filesystems import COMPRESSION_FILESYSTEMS\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\filesystems\\__init__.py\", line 16, in <module>\n",
      "    from .s3filesystem import S3FileSystem  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\datasets\\filesystems\\s3filesystem.py\", line 1, in <module>\n",
      "    import s3fs\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\s3fs\\__init__.py\", line 1, in <module>\n",
      "    from .core import S3FileSystem, S3File\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\s3fs\\core.py\", line 29, in <module>\n",
      "    import aiobotocore.session\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\aiobotocore\\session.py\", line 12, in <module>\n",
      "    from .client import AioBaseClient, AioClientCreator\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\aiobotocore\\client.py\", line 18, in <module>\n",
      "    from .args import AioClientArgsCreator\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\aiobotocore\\args.py\", line 8, in <module>\n",
      "    from .endpoint import AioEndpointCreator\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\aiobotocore\\endpoint.py\", line 18, in <module>\n",
      "    from aiobotocore.httpchecksum import handle_checksum_body\n",
      "  File \"c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\aiobotocore\\httpchecksum.py\", line 3, in <module>\n",
      "    from botocore.httpchecksum import (\n",
      "ImportError: cannot import name 'conditionally_calculate_md5' from 'botocore.httpchecksum' (c:\\Users\\MSI\\anaconda3\\Lib\\site-packages\\botocore\\httpchecksum.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2052, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "                                              ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MSI\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be74511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Embedding Wrapper (from earlier) ---\n",
    "class LangchainSentenceTransformer:\n",
    "    def __init__(self, model_name: str = \"mixedbread-ai/mxbai-embed-large-v1\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name).to(self.device)\n",
    "        self.query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "        self._cleaned_documents = []\n",
    "\n",
    "    def embed_documents(self, documents: list[Document]) -> tuple[list[str], np.ndarray]:\n",
    "        # Simple filter to drop empty or irrelevant chunks\n",
    "        bad_substrings = [\"Atomic Structure Representation\", \"Logical path\\nD\\nF\\nC\\nA\\nE\"]\n",
    "        filtered: list[Document] = []\n",
    "        for doc in documents:\n",
    "            text = doc.page_content.strip()\n",
    "            if text and not any(bad in text for bad in bad_substrings):\n",
    "                filtered.append(Document(page_content=text, metadata=doc.metadata))\n",
    "        self._cleaned_documents = filtered\n",
    "        texts = [d.page_content for d in filtered]\n",
    "\n",
    "        # Encode all chunks\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        return texts, embeddings\n",
    "\n",
    "    def embed_query(self, query: str) -> np.ndarray:\n",
    "        return self.model.encode(\n",
    "            [self.query_prefix + query],\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device\n",
    "        )[0]\n",
    "\n",
    "# --- Configuration ---\n",
    "ROOT_DIR = Path(\"clean_txt\")     # root folder containing subfolders '02', '03', ..., '12'\n",
    "CHUNK_SIZE = 1000\n",
    "overlap = 200\n",
    "\n",
    "# --- Collect Documents ---\n",
    "all_docs: list[Document] = []\n",
    "for month_dir in sorted(ROOT_DIR.iterdir()):\n",
    "    if not month_dir.is_dir():\n",
    "        continue\n",
    "    for txt_file in sorted(month_dir.glob(\"*.txt\")):\n",
    "        text = txt_file.read_text(encoding=\"utf-8\")\n",
    "        # store metadata if desired\n",
    "        meta = {\"file\": txt_file.stem, \"month\": month_dir.name}\n",
    "        all_docs.append(Document(page_content=text, metadata=meta))\n",
    "\n",
    "print(f\"[INFO] Loaded {len(all_docs)} source documents.\")\n",
    "\n",
    "# --- Chunking ---\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=overlap\n",
    ")\n",
    "chunked_docs: list[Document] = []\n",
    "for doc in all_docs:\n",
    "    chunks = splitter.split_documents([doc])\n",
    "    chunked_docs.extend(chunks)\n",
    "print(f\"[INFO] Split into {len(chunked_docs)} chunks of ~{CHUNK_SIZE} chars each.\")\n",
    "\n",
    "# --- Embedding ---\n",
    "embedder = LangchainSentenceTransformer()\n",
    "texts, embeddings = embedder.embed_documents(chunked_docs)\n",
    "print(f\"[INFO] Produced {len(texts)} embeddings of dimension {embeddings.shape[1]}\")\n",
    "\n",
    "# --- (Optional) Save embeddings for later ---\n",
    "# np.savez_compressed(\"fallos_embeddings.npz\", texts=texts, embeddings=embeddings)\n",
    "\n",
    "# Now `texts` and `embeddings` are ready for indexing in FAISS or pgvector.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
