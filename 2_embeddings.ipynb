{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ecb8cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxi/miniforge3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, glob, pathlib\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings   # o HuggingFace\n",
    "from langchain.vectorstores import FAISS            # o Chroma, Milvus…\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80a438ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONS_FOLDER = \"datasets/fallos_json\"\n",
    "OUTPUT_FOLDER = \"datasets/fallos_vectorstore\"\n",
    "\n",
    "EMBEDDINGS_MODEL = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c161080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_cases(folder=JSONS_FOLDER):\n",
    "    \"\"\"Devuelve una lista de dicts {'case_id', 'section', 'text'}.\"\"\"\n",
    "    rows = []\n",
    "    for path in pathlib.Path(folder).glob(\"*.json\"):\n",
    "        data = json.loads(path.read_text())[0]       # un fallo por archivo\n",
    "        case = path.stem                             # ej. 8104\n",
    "        for section, lines in data[\"CONTENIDO\"].items():\n",
    "            # Unimos las líneas originales\n",
    "            section_txt = \"\\n\".join(lines).strip()\n",
    "            rows.append({\"case_id\": case,\n",
    "                         \"section\": section,\n",
    "                         \"text\": section_txt})\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c624ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total chunks: 253\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,             \n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for row in load_cases():\n",
    "    for chunk_id, chunk in enumerate(splitter.split_text(row[\"text\"])):\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"case_id\": row[\"case_id\"],\n",
    "                    \"section\": row[\"section\"],\n",
    "                    \"chunk_id\": chunk_id\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "print(f\"[INFO] Total chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ca65a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial\n",
    "# embeddings_wrapper.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "class LangchainSentenceTransformer:\n",
    "    \"\"\"\n",
    "    Wrapper simple para usar un modelo open-source de Sentence-Transformers\n",
    "    dentro de LangChain (sólo lo que FAISS necesita).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"mixedbread-ai/mxbai-embed-large-v1\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model  = SentenceTransformer(model_name).to(self.device)\n",
    "\n",
    "        # prefijo opcional sugerido por el paper \"bge\" / MixedBread\n",
    "        self.query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "\n",
    "        # almacena docs limpios para referencia\n",
    "        self._cleaned_documents: list[Document] = []\n",
    "\n",
    "    # --- Métodos requeridos por LangChain -----------------\n",
    "    def embed_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Recibe una lista de langchain.Document y devuelve (texts, np.ndarray[n_docs, dim])\n",
    "        \"\"\"\n",
    "        # 1) limpieza rápida opcional  ──> adaptala si lo necesitás\n",
    "        bad = [\"Atomic Structure Representation\", \"Logical path\\nD\\nF\\nC\\nA\\nE\"]\n",
    "        filtered = [\n",
    "            Document(page_content=doc.page_content.strip())\n",
    "            for doc in documents\n",
    "            if doc.page_content.strip() and not any(b in doc.page_content for b in bad)\n",
    "        ]\n",
    "        self._cleaned_documents = filtered\n",
    "\n",
    "        # 2) embed\n",
    "        texts = [d.page_content for d in filtered]\n",
    "        vectors = self.model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return texts, vectors\n",
    "\n",
    "    def embed_query(self, query: str):\n",
    "        \"\"\"\n",
    "        Embedding de una sola consulta (devuelve np.ndarray[dim])\n",
    "        \"\"\"\n",
    "        vec = self.model.encode(\n",
    "            [self.query_prefix + str(query)],\n",
    "            convert_to_numpy=True,\n",
    "            device=self.device,\n",
    "        )[0]\n",
    "        return vec\n",
    "\n",
    "    # hace al wrapper \"callable\" dentro de LangChain si se lo pasa como función\n",
    "    def __call__(self, text: str):\n",
    "        return self.embed_query(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96337609",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder           = LangchainSentenceTransformer(\n",
    "                        model_name=\"mixedbread-ai/mxbai-embed-large-v1\"  \n",
    "                    )\n",
    "texts, vectors     = embedder.embed_documents(docs)   # devuelve np.ndarray\n",
    "pairs              = list(zip(texts, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5048a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de búsqueda\n",
    "retriever = vectordb.as_retriever(search_k=5)\n",
    "query = \"constitución en mora artículo 1849\"\n",
    "for doc in retriever.get_relevant_documents(query):\n",
    "    print(doc.metadata)   # -> {'case_id': '8344', 'section': 'GISELA N. SCHUMACHER DIJO', ...}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
