{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780d0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'main_source_cited_articles_stats.json' generado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Ruta base donde buscar los archivos .json\n",
    "base_dir = \"../datasets/fallos_json\"\n",
    "\n",
    "# Diccionario para almacenar la informaci√≥n\n",
    "result = {}\n",
    "\n",
    "# Recorrer todas las carpetas y archivos .json\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    citations = (\n",
    "                        data.get(\"METADATOS\", {})\n",
    "                        .get(\"ARTICULOS_CITADOS\", {})\n",
    "                        .get(\"citations\", [])\n",
    "                    )\n",
    "                    for citation in citations:\n",
    "                        main_source = citation.get(\"main_source\")\n",
    "                        cited_articles = citation.get(\"cited_articles\", [])\n",
    "                        if main_source is not None and cited_articles is not None:\n",
    "                            if main_source not in result:\n",
    "                                result[main_source] = []\n",
    "                            result[main_source].extend(cited_articles)\n",
    "            except Exception as e:\n",
    "                print(f\"Error en {file_path}: {e}\")\n",
    "\n",
    "# Construir el JSON de salida\n",
    "output = {}\n",
    "for source, articles in result.items():\n",
    "    freq = Counter(articles)\n",
    "    output[source] = {\n",
    "        \"cited_articles_frequency\": dict(freq),\n",
    "        \"total_cited_articles\": sum(freq.values())\n",
    "    }\n",
    "\n",
    "# Guardar el resultado en un archivo\n",
    "with open(\"main_source_cited_articles_stats.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Archivo 'main_source_cited_articles_stats.json' generado correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51915c3e",
   "metadata": {},
   "source": [
    "# Prueba del procesador enriquecido de fallos legales\n",
    "\n",
    "Vamos a procesar los archivos JSON usando el nuevo procesador y mostrar algunos fragmentos enriquecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0035b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de fragmentos extra√≠dos: 578\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from typing import Iterator, Set, Dict, Any\n",
    "from pydantic import ValidationError\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LegalParagraphEnriched(BaseModel):\n",
    "    \"\"\"Modelo enriquecido para p√°rrafos legales con contexto adicional\"\"\"\n",
    "    expediente: str = Field(..., description=\"N√∫mero de expediente\")\n",
    "    section: str = Field(..., description=\"Secci√≥n del documento\")\n",
    "    paragraph_id: int = Field(..., ge=0, description=\"ID del p√°rrafo dentro de la secci√≥n\")\n",
    "    text: str = Field(..., min_length=1, description=\"Contenido del p√°rrafo\")\n",
    "    path: str = Field(..., description=\"Ruta relativa del archivo fuente\")\n",
    "    idea_central: Optional[str] = Field(None, description=\"Idea central del fallo\")\n",
    "    articulos_citados: Optional[List[Dict[str, Any]]] = Field(None, description=\"Art√≠culos citados en el fallo\")\n",
    "    materia_preliminar: Optional[str] = Field(None, description=\"Materia preliminar del fallo\")\n",
    "    metadatos: Optional[Dict[str, Any]] = Field(None, description=\"Metadatos adicionales del fallo\")\n",
    "\n",
    "    def to_search_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convierte a formato para b√∫squeda\"\"\"\n",
    "        return {\n",
    "            \"expte\": self.expediente,\n",
    "            \"section\": self.section,\n",
    "            \"paragraph\": self.text,\n",
    "            \"path\": self.path,\n",
    "            \"paragraph_id\": self.paragraph_id,\n",
    "            \"idea_central\": self.idea_central,\n",
    "            \"articulos_citados\": self.articulos_citados,\n",
    "            \"materia_preliminar\": self.materia_preliminar,\n",
    "            \"metadatos\": self.metadatos\n",
    "        }\n",
    "\n",
    "\n",
    "class EnrichedProcessor:\n",
    "    \"\"\"Procesador que extrae fragmentos enriquecidos de los fallos JSON\"\"\"\n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            \"files_processed\": 0,\n",
    "            \"paragraphs_extracted\": 0,\n",
    "            \"expedientes_found\": set(),\n",
    "            \"errors\": []\n",
    "        }\n",
    "\n",
    "    def process_directory(self, json_dir: Path) -> Iterator[LegalParagraphEnriched]:\n",
    "        \"\"\"Procesa todos los archivos JSON en el directorio y subdirectorios\"\"\"\n",
    "        json_files = list(json_dir.rglob(\"*.json\"))\n",
    "        logger.info(f\"üìÑ Encontrados {len(json_files)} archivos\")\n",
    "        for file_path in json_files:\n",
    "            try:\n",
    "                yield from self._process_file(file_path, json_dir)\n",
    "                self.stats[\"files_processed\"] += 1\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error en {file_path.name}: {str(e)}\"\n",
    "                logger.error(error_msg)\n",
    "                self.stats[\"errors\"].append(error_msg)\n",
    "                continue\n",
    "        logger.info(f\"‚úÖ Procesamiento completado: {self.stats}\")\n",
    "\n",
    "    def _process_file(self, file_path: Path, base_dir: Path) -> Iterator[LegalParagraphEnriched]:\n",
    "        content = file_path.read_text(encoding=\"utf-8\")\n",
    "        docs = json.loads(content)\n",
    "        if not isinstance(docs, list):\n",
    "            docs = [docs]\n",
    "        for doc in docs:\n",
    "            yield from self._process_document(doc, file_path, base_dir)\n",
    "\n",
    "    def _process_document(self, doc: Dict[str, Any], file_path: Path, base_dir: Path) -> Iterator[LegalParagraphEnriched]:\n",
    "        contenido = doc[\"CONTENIDO\"]\n",
    "        expte = self._extract_expediente(contenido[\"INICIO\"][0])\n",
    "        idea_central = doc.get(\"IDEA_CENTRAL\")\n",
    "        articulos_citados = doc.get(\"METADATOS\", {}).get(\"ARTICULOS_CITADOS\", {}).get(\"citations\", [])\n",
    "        materia_preliminar = doc.get(\"MATERIA_PRELIMINAR\")\n",
    "        metadatos = doc.get(\"METADATOS\", {})\n",
    "        self.stats[\"expedientes_found\"].add(expte)\n",
    "        for section, paragraphs in contenido.items():\n",
    "            if not isinstance(paragraphs, list):\n",
    "                continue\n",
    "            for idx, text in enumerate(paragraphs):\n",
    "                if not text or not isinstance(text, str) or len(text.strip()) < 10:\n",
    "                    continue\n",
    "                try:\n",
    "                    paragraph = LegalParagraphEnriched(\n",
    "                        expediente=expte,\n",
    "                        section=section,\n",
    "                        paragraph_id=idx,\n",
    "                        text=text,\n",
    "                        path=file_path.relative_to(base_dir).as_posix(),\n",
    "                        idea_central=idea_central,\n",
    "                        articulos_citados=articulos_citados,\n",
    "                        materia_preliminar=materia_preliminar,\n",
    "                        metadatos=metadatos\n",
    "                    )\n",
    "                    self.stats[\"paragraphs_extracted\"] += 1\n",
    "                    yield paragraph\n",
    "                except ValidationError:\n",
    "                    continue\n",
    "\n",
    "    def _extract_expediente(self, header: str) -> str:\n",
    "        for pattern in [\"Expte. N¬∫\", \"Expediente\", \"Exp.\"]:\n",
    "            if pattern in header:\n",
    "                return header.split(pattern)[-1].strip().split()[0]\n",
    "        import hashlib\n",
    "        return hashlib.md5(header.encode()).hexdigest()[:8].upper()\n",
    "\n",
    "    def get_stats(self) -> dict:\n",
    "        return {\n",
    "            \"processor_type\": \"enriched\",\n",
    "            \"files_processed\": self.stats[\"files_processed\"],\n",
    "            \"paragraphs_extracted\": self.stats[\"paragraphs_extracted\"],\n",
    "            \"expedientes_found\": len(self.stats[\"expedientes_found\"]),\n",
    "            \"errors\": len(self.stats[\"errors\"]),\n",
    "            \"error_rate\": len(self.stats[\"errors\"]) / max(1, self.stats[\"files_processed\"])\n",
    "        }\n",
    "\n",
    "\n",
    "# Ruta al directorio de fallos JSON\n",
    "json_dir = Path(\"datasets/fallos_json/2024/07\")\n",
    "\n",
    "# Instanciar el procesador enriquecido\n",
    "processor = EnrichedProcessor()\n",
    "\n",
    "# Procesar el directorio y obtener los fragmentos enriquecidos\n",
    "fragmentos = list(processor.process_directory(json_dir))\n",
    "\n",
    "print(f\"Total de fragmentos extra√≠dos: {len(fragmentos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba46fb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expte: 8554\n",
      "Secci√≥n: INICIO\n",
      "Texto: \"FIGUEROA MERCEDES NOEMI POR SI Y EN NOMBRE Y REPRESENTACI√ìN DE SUS HIJOS MENORES DE EDAD C/ FONTANINI CRISTIAN MATIAS Y OTROS S/ ORDINARIO ACCIDENTES DE TRANSITO\" - Expte. N¬∫ 8554\n",
      "Idea central: **IDEA CENTRAL DEL FALLO:**\n",
      "\n",
      "El Tribunal declar√≥ inadmisible el recurso de inaplicabilidad de ley in\n",
      "Art√≠culos citados: [{'main_source': 'Ley 24.449', 'cited_articles': [68], 'extra': None}, {'main_source': 'Ley 7046', 'cited_articles': [64, 3, 30, 63, 94], 'extra': 'modif. por ley 11.141'}, {'main_source': 'C√≥digo Procesal Civil y Comercial (CPCC)', 'cited_articles': [65, 277, 280], 'extra': None}, {'main_source': 'Acuerdo General 15/18 SNE', 'cited_articles': [1, 4], 'extra': None}, {'main_source': 'Acuerdo General 11/20', 'cited_articles': [0], 'extra': 'del 23-6-2020'}, {'main_source': 'Otro', 'cited_articles': [56, 118, 29, 28, 114], 'extra': 'p√°rr. 3, de la Ley 17.148'}]\n",
      "Materia: ORDINARIO ACCIDENTES DE TRANSITO\n",
      "------------------------------------------------------------\n",
      "Expte: 8554\n",
      "Secci√≥n: ACUERDO\n",
      "Texto: En la ciudad de Paran√°, capital de la provincia de Entre R√≠os, a los veintid√≥s d√≠as del mes de julio del a√±o dos mil veinticuatro reunidos los integrantes de este Tribunal asistidos por el Secretario autorizante, para conocer el recurso de inaplicabilidad de ley deducido en fecha 7/7/2023 en las actuaciones: \"FIGUEROA MERCEDES NOEMI POR SI Y EN NOMBRE Y REPRESENTACI√ìN DE SUS HIJOS MENORES DE EDAD C/ FONTANINI CRISTIAN MATIAS Y OTROS S/ ORDINARIO ACCIDENTES DE TRANSITO\" - Expte. N¬∫ 8554, respecto de la resoluci√≥n de la Sala Tercera de la C√°mara Segunda de Apelaciones de Paran√° dictada en fecha 22/6/2023. Que la votaci√≥n debe tener lugar en el siguiente orden: Sr.\n",
      "Idea central: **IDEA CENTRAL DEL FALLO:**\n",
      "\n",
      "El Tribunal declar√≥ inadmisible el recurso de inaplicabilidad de ley in\n",
      "Art√≠culos citados: [{'main_source': 'Ley 24.449', 'cited_articles': [68], 'extra': None}, {'main_source': 'Ley 7046', 'cited_articles': [64, 3, 30, 63, 94], 'extra': 'modif. por ley 11.141'}, {'main_source': 'C√≥digo Procesal Civil y Comercial (CPCC)', 'cited_articles': [65, 277, 280], 'extra': None}, {'main_source': 'Acuerdo General 15/18 SNE', 'cited_articles': [1, 4], 'extra': None}, {'main_source': 'Acuerdo General 11/20', 'cited_articles': [0], 'extra': 'del 23-6-2020'}, {'main_source': 'Otro', 'cited_articles': [56, 118, 29, 28, 114], 'extra': 'p√°rr. 3, de la Ley 17.148'}]\n",
      "Materia: ORDINARIO ACCIDENTES DE TRANSITO\n",
      "------------------------------------------------------------\n",
      "Expte: 8554\n",
      "Secci√≥n: ACUERDO\n",
      "Texto: Vocal Carlos Federico Tepsich; Sra. Vocal Gisela N. Schumacher y Sr. Vocal Leonardo Portela.\n",
      "Idea central: **IDEA CENTRAL DEL FALLO:**\n",
      "\n",
      "El Tribunal declar√≥ inadmisible el recurso de inaplicabilidad de ley in\n",
      "Art√≠culos citados: [{'main_source': 'Ley 24.449', 'cited_articles': [68], 'extra': None}, {'main_source': 'Ley 7046', 'cited_articles': [64, 3, 30, 63, 94], 'extra': 'modif. por ley 11.141'}, {'main_source': 'C√≥digo Procesal Civil y Comercial (CPCC)', 'cited_articles': [65, 277, 280], 'extra': None}, {'main_source': 'Acuerdo General 15/18 SNE', 'cited_articles': [1, 4], 'extra': None}, {'main_source': 'Acuerdo General 11/20', 'cited_articles': [0], 'extra': 'del 23-6-2020'}, {'main_source': 'Otro', 'cited_articles': [56, 118, 29, 28, 114], 'extra': 'p√°rr. 3, de la Ley 17.148'}]\n",
      "Materia: ORDINARIO ACCIDENTES DE TRANSITO\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 3 fragmentos enriquecidos\n",
    "for frag in fragmentos[:3]:\n",
    "    print(f\"Expte: {frag.expediente}\")\n",
    "    print(f\"Secci√≥n: {frag.section}\")\n",
    "    print(f\"Texto: {frag.text}\")\n",
    "    print(f\"Idea central: {frag.idea_central[:100] if frag.idea_central else None}\")\n",
    "    print(f\"Art√≠culos citados: {frag.articulos_citados}\")\n",
    "    print(f\"Materia: {frag.materia_preliminar}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7d275b",
   "metadata": {},
   "source": [
    "Si ves los campos extra correctamente poblados, ¬°el procesador est√° funcionando bien!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
