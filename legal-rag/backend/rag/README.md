# üß† M√≥dulo RAG

M√≥dulo encargado del **pipeline RAG (Retrieval-Augmented Generation)** para consultas legales. Orquesta la b√∫squeda de informaci√≥n relevante y la generaci√≥n de respuestas usando LLMs, proporcionando diferentes estrategias como RAG est√°ndar y conversacional.

## üèóÔ∏è Arquitectura

```
backend/rag/
‚îú‚îÄ‚îÄ __init__.py          # Exports principales
‚îú‚îÄ‚îÄ base.py              # Interface base
‚îú‚îÄ‚îÄ factory.py           # Factory para pipelines
‚îî‚îÄ‚îÄ strategies/
    ‚îú‚îÄ‚îÄ __init__.py      # Exports de estrategias
    ‚îú‚îÄ‚îÄ standard.py      # RAG est√°ndar con singletons
    ‚îî‚îÄ‚îÄ conversational.py # RAG con memoria conversacional
```

## üìÅ Archivos

### [`base.py`](base.py)
**Interface base para todos los pipelines RAG:**
```python
class BaseRAGPipeline(ABC):
    @abstractmethod
    def query(self, question: str, top_n: int = 8) -> Tuple[str, List[Dict[str, Any]]]:
        pass
    
    def get_stats(self) -> Dict[str, Any]:
        return {"pipeline_type": "base"}
    
    def supports_streaming(self) -> bool:
        return False
```

### [`factory.py`](factory.py)
**Factory principal para crear pipelines RAG:**
- `get_rag_pipeline(strategy)` - Crea pipeline seg√∫n estrategia
- `answer(question, top_n)` - Funci√≥n de conveniencia
- `get_available_strategies()` - Lista estrategias disponibles
- `get_default_strategy()` - Estrategia por defecto
- **Lazy imports** para mejor rendimiento

### [`strategies/standard.py`](strategies/standard.py)
**Pipeline RAG est√°ndar optimizado:**
- ‚úÖ **Singletons globales** para retriever y LLM (evita re-inicializaci√≥n)
- ‚úÖ **M√©tricas detalladas** de tiempo por componente
- ‚úÖ **Prompt especializado** para asistente jur√≠dico
- ‚úÖ **Configuraci√≥n flexible** via env vars
- ‚úÖ **Contexto optimizado** con truncado inteligente
- ‚úÖ **Logging performance** completo

### [`strategies/conversational.py`](strategies/conversational.py)
**Pipeline RAG con memoria conversacional:**
- ‚úÖ **Historial de conversaci√≥n** configurable
- ‚úÖ **Contexto combinado** (historial + nueva evidencia)
- ‚úÖ **Prompt conversacional** especializado
- ‚úÖ **Gesti√≥n autom√°tica** de memoria (l√≠mite de turnos)
- ‚úÖ **Clear history** manual
- ‚úÖ **Streaming support** preparado

## üöÄ Uso B√°sico

```python
# Opci√≥n 1: Factory (recomendado)
from backend.rag import get_rag_pipeline
pipeline = get_rag_pipeline("standard")
response, hits = pipeline.query("¬øQu√© dice sobre contratos?", top_n=5)

# Opci√≥n 2: Funci√≥n de conveniencia
from backend.rag import answer
response, hits = answer("¬øQu√© dice sobre contratos?")

# Opci√≥n 3: Pipeline conversacional
pipeline = get_rag_pipeline("conversational", max_history=10)
response1, hits1 = pipeline.query("¬øQu√© es un contrato?")
response2, hits2 = pipeline.query("¬øY qu√© obligaciones genera?")  # Usa contexto previo
pipeline.clear_history()  # Limpiar cuando sea necesario

# Opci√≥n 4: Acceso directo
from backend.rag import StandardRAGPipeline, ConversationalRAGPipeline
pipeline = StandardRAGPipeline()
conv_pipeline = ConversationalRAGPipeline(max_history=5)

# Ver estrategias disponibles
from backend.rag import get_available_strategies
print("Disponibles:", get_available_strategies())

# Estad√≠sticas del pipeline
stats = pipeline.get_stats()
print(f"Retriever cargado: {stats['retriever_loaded']}")
```

## ‚öôÔ∏è Configuraci√≥n

```bash
# Estrategia RAG
export RAG_STRATEGY=standard           # standard | conversational

# Configuraci√≥n StandardRAG
export MAX_PARAGRAPH_LENGTH=300        # Caracteres m√°ximos por p√°rrafo
export LLM_MAX_TOKENS=300              # Tokens m√°ximos para respuesta
export MAX_RESULTS_PER_QUERY=8         # Resultados m√°ximos de b√∫squeda

# Configuraci√≥n m√≥dulos dependientes
export SEARCH_STRATEGY=hybrid          # Estrategia de b√∫squeda
export LLM_PROVIDER=azure              # Proveedor LLM
```

## üîß Agregar Nueva Estrategia

### 1. Crear nueva implementaci√≥n:

```python
# strategies/multi_step.py
import time
from ..base import BaseRAGPipeline
from backend.search import get_retriever
from backend.llm import get_llm_provider

class MultiStepRAGPipeline(BaseRAGPipeline):
    """Pipeline RAG con reasoning multi-paso"""
    
    def __init__(self, max_steps: int = 3):
        self.max_steps = max_steps
        self.retriever = None
        self.llm_provider = None
    
    def query(self, question: str, top_n: int = 8):
        """RAG con multiple pasos de reasoning"""
        if self.retriever is None:
            self.retriever = get_retriever("hybrid")
        if self.llm_provider is None:
            self.llm_provider = get_llm_provider("azure")
        
        steps = []
        current_question = question
        
        for step in range(self.max_steps):
            # 1. B√∫squeda para el paso actual
            hits = self.retriever.query(current_question, top_n)
            
            # 2. Generar sub-respuesta
            context = self._build_context(hits)
            sub_response = self._generate_step_response(
                current_question, context, step + 1
            )
            
            steps.append({
                "step": step + 1,
                "question": current_question,
                "response": sub_response,
                "hits": hits
            })
            
            # 3. Generar siguiente pregunta (si no es √∫ltimo paso)
            if step < self.max_steps - 1:
                current_question = self._generate_next_question(
                    question, steps
                )
        
        # 4. Combinar todos los pasos en respuesta final
        final_response = self._combine_steps(question, steps)
        all_hits = [hit for step in steps for hit in step["hits"]]
        
        return final_response, all_hits[:top_n]
    
    def _build_context(self, hits):
        return "\n".join(f"[{h['expte']}]: {h['paragraph'][:200]}" for h in hits)
    
    def _generate_step_response(self, question, context, step_num):
        prompt = f"""
        Paso {step_num} de an√°lisis legal:
        Pregunta: {question}
        Contexto: {context}
        
        Analiza este aspecto espec√≠fico:
        """
        messages = [{"role": "user", "content": prompt}]
        return self.llm_provider.generate(messages, max_tokens=200)
    
    def _generate_next_question(self, original_question, steps):
        # L√≥gica para generar siguiente pregunta basada en pasos previos
        prompt = f"""
        Pregunta original: {original_question}
        An√°lisis previo: {steps[-1]['response'][:100]}...
        
        ¬øQu√© aspecto legal espec√≠fico deber√≠a analizar a continuaci√≥n?
        Responde solo la pregunta espec√≠fica:
        """
        messages = [{"role": "user", "content": prompt}]
        return self.llm_provider.generate(messages, max_tokens=50)
    
    def _combine_steps(self, original_question, steps):
        # Combinar todos los pasos en respuesta final coherente
        combined_analysis = "\n\n".join([
            f"**Paso {s['step']}:** {s['response']}" for s in steps
        ])
        
        prompt = f"""
        Pregunta original: {original_question}
        
        An√°lisis multi-paso realizado:
        {combined_analysis}
        
        Proporciona respuesta final integrada en formato de tabla Markdown:
        """
        messages = [{"role": "user", "content": prompt}]
        return self.llm_provider.generate(messages, max_tokens=400)
    
    def get_stats(self):
        return {
            "pipeline_type": "multi_step",
            "max_steps": self.max_steps,
            "retriever_loaded": self.retriever is not None,
            "llm_provider_loaded": self.llm_provider is not None
        }
    
    def supports_streaming(self):
        return True
```

### 2. Registrar en factory:

```python
# factory.py
def _import_multi_step():
    from .strategies.multi_step import MultiStepRAGPipeline
    return MultiStepRAGPipeline

def get_rag_pipeline(strategy: str = None, **kwargs):
    strategies = {
        "standard": lambda: _import_standard(),
        "conversational": lambda: _import_conversational(),
        "multi_step": lambda: _import_multi_step(),  # ‚Üê Agregar
    }
    # ... resto del c√≥digo

def get_available_strategies():
    return ["standard", "conversational", "multi_step"]  # ‚Üê Agregar
```

### 3. Exportar en strategies/__init__.py:

```python
# strategies/__init__.py
from .standard import StandardRAGPipeline
from .conversational import ConversationalRAGPipeline
from .multi_step import MultiStepRAGPipeline  # ‚Üê Agregar

__all__ = [
    "StandardRAGPipeline",
    "ConversationalRAGPipeline",
    "MultiStepRAGPipeline"  # ‚Üê Agregar
]
```

### 4. Usar nueva estrategia:

```python
# Uso directo
from backend.rag import get_rag_pipeline
pipeline = get_rag_pipeline("multi_step", max_steps=5)

# Via environment variable
export RAG_STRATEGY=multi_step
pipeline = get_rag_pipeline()  # Usa multi_step autom√°ticamente
```

## üìä Ejemplo de Salida

```python
# Respuesta StandardRAG
response = """
| # | Expte. | Secci√≥n | Extracto (m√°x 40 palabras) |
|---|--------|---------|----------------------------|
| 1 | 123/2024 | FUNDAMENTOS | El contrato celebrado entre las partes establece obligaciones rec√≠procas... |
| 2 | 456/2024 | CONSIDERANDO | Los contratos deben cumplirse de buena fe seg√∫n art. 961 CCyC... |
"""

# Hits de b√∫squeda
hits = [
    {
        "score": 0.8234,
        "expte": "123/2024", 
        "section": "FUNDAMENTOS",
        "paragraph": "El contrato celebrado entre las partes...",
        "path": "fallos/2024/enero/fallo_123.json"
    }
]

# Estad√≠sticas StandardRAG
stats = {
    "pipeline_type": "standard",
    "retriever_loaded": True,
    "llm_provider_loaded": True,
    "max_paragraph_length": 300,
    "max_tokens": 300,
    "max_results": 8,
    "retriever_stats": {"retriever_type": "hybrid", ...},
    "llm_stats": {"provider_type": "azure", ...}
}

# Logs de performance
üìä StandardRAG query processed in 1.234s:
   Search: 0.456s, Context: 0.012s, LLM: 0.766s
   Results: 8, Context length: 1847 chars
```

## üéØ Caracter√≠sticas Clave

- **üîÑ Factory Pattern**: F√°cil cambio entre estrategias
- **üöÄ Singletons Optimizados**: Evita re-cargar modelos pesados
- **üìä M√©tricas Detalladas**: Logging completo de performance
- **üó£Ô∏è Memoria Conversacional**: Contexto multi-turn autom√°tico
- **‚öôÔ∏è Configurable**: Todo v√≠a variables de entorno
- **üéØ Prompts Especializados**: Para contexto jur√≠dico
- **üîß Extensible**: F√°cil agregar nuevas estrategias
- **üìà Performance Tracking**: Tiempos por componente

## üí° Estrategias Futuras

### CitationRAGPipeline
```python
class CitationRAGPipeline(BaseRAGPipeline):
    """RAG con citas precisas y verificaci√≥n de fuentes"""
    
    def query(self, question, top_n=8):
        # Generar respuesta con citas verificables
        # Validar que cada afirmaci√≥n tenga respaldo
        # Formato: "Seg√∫n expediente X, secci√≥n Y: [afirmaci√≥n]"
        pass
```

### FactCheckRAGPipeline  
```python
class FactCheckRAGPipeline(BaseRAGPipeline):
    """RAG con verificaci√≥n autom√°tica de hechos"""
    
    def query(self, question, top_n=8):
        # 1. RAG normal
        # 2. Extraer afirmaciones clave
        # 3. Verificar cada afirmaci√≥n contra corpus
        # 4. Marcar nivel de confianza
        pass
```

### SummarizeRAGPipeline
```python
class SummarizeRAGPipeline(BaseRAGPipeline):
    """RAG especializado en res√∫menes de jurisprudencia"""
    
    def query(self, question, top_n=20):
        # Recuperar m√°s documentos
        # Agrupar por tema/a√±o/tipo
        # Generar resumen consolidado
        pass
```

## ‚ö†Ô∏è Notas Importantes

- **Singletons**: Los retrievers y LLM providers se reutilizan para eficiencia
- **Memoria**: ConversationalRAG mantiene estado entre consultas
- **Performance**: StandardRAG t√≠picamente 1-2 segundos por consulta
- **Context Length**: Cuidar l√≠mites de contexto del LLM (300 tokens por defecto)
- **Dependencies**: Requiere backend.search y backend.llm configurados
- **Prompts**: Especializados para contexto jur√≠dico argentino
- **Lazy Loading**: Componentes se cargan solo cuando se necesitan