{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74237701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re, os, argparse, pdfplumber\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "for name in (\"pdfminer\", \"pdfminer.layout\", \"pdfminer.pdfpage\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29fcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "\n",
    "\n",
    "# Prueba inicial con solo un mes (febrero 2024 - 10 fallos)\n",
    "PATH_FALLOS = Path(os.path.join(PATH, \"2024/02\"))\n",
    "PATH_RECORTES = Path(os.path.join(PATH, \"cropped_pdfs\"))\n",
    "PATH_TXT = Path(os.path.join(PATH, \"fallos_txts\"))\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "\n",
    "os.makedirs(PATH_RECORTES, exist_ok=True)\n",
    "os.makedirs(PATH_TXT, exist_ok=True)\n",
    "os.makedirs(PATH_JSON, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10af937",
   "metadata": {},
   "source": [
    "## Recorte de PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bb6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8927.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8926.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8104.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8865.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8142.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8948.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8752.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8971.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8569.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8344.pdf\n"
     ]
    }
   ],
   "source": [
    "HEADER_PT = 82      # quitar SOLO en páginas impares (arriba)\n",
    "FOOTER_PT = 50      # quitar SIEMPRE (abajo)\n",
    "\n",
    "for pdf in PATH_FALLOS.glob(\"*.pdf\"):\n",
    "    reader, writer = PdfReader(str(pdf)), PdfWriter()\n",
    "    for idx, page in enumerate(reader.pages, start=1):\n",
    "        box = page.cropbox                       # caja de recorte base\n",
    "        # -- recorte inferior --\n",
    "        box.lower_left  = (box.lower_left[0],  box.lower_left[1] + FOOTER_PT)\n",
    "        # -- recorte superior en páginas impares --\n",
    "        if idx % 2 == 1:\n",
    "            box.upper_right = (box.upper_right[0], box.upper_right[1] - HEADER_PT)\n",
    "\n",
    "        # aplicar a todos los bounding-boxes que respetan los visores/lectores\n",
    "        page.cropbox  = box\n",
    "        page.trimbox  = box\n",
    "        page.mediabox = box\n",
    "        writer.add_page(page)\n",
    "\n",
    "    out_file = PATH_RECORTES / pdf.name\n",
    "    with out_file.open(\"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "    print(\"✔ Cropped →\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecba0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8927.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8926.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8104.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8865.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8142.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8948.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8752.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8971.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8569.pdf\n",
      "✔ Cropped → /Users/maxi/Downloads/Materias Actuales/NLP/TP_NLP/datasets/cropped_pdfs/8344.pdf\n"
     ]
    }
   ],
   "source": [
    "HEADER_PT = 82      # quitar SOLO en páginas impares (arriba)\n",
    "FOOTER_PT = 50      # quitar SIEMPRE (abajo)\n",
    "\n",
    "for pdf in PATH_FALLOS.rglob(\"*.pdf\"):  # Cambio aquí: .rglob() en lugar de .glob()\n",
    "    reader, writer = PdfReader(str(pdf)), PdfWriter()\n",
    "    for idx, page in enumerate(reader.pages, start=1):\n",
    "        box = page.cropbox                       # caja de recorte base\n",
    "        # -- recorte inferior --\n",
    "        box.lower_left  = (box.lower_left[0],  box.lower_left[1] + FOOTER_PT)\n",
    "        # -- recorte superior en páginas impares --\n",
    "        if idx % 2 == 1:\n",
    "            box.upper_right = (box.upper_right[0], box.upper_right[1] - HEADER_PT)\n",
    "\n",
    "        # aplicar a todos los bounding-boxes que respetan los visores/lectores\n",
    "        page.cropbox  = box\n",
    "        page.trimbox  = box\n",
    "        page.mediabox = box\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Mantener la estructura de subcarpetas en el output\n",
    "    rel_path = pdf.relative_to(PATH_FALLOS)\n",
    "    out_file = PATH_RECORTES / rel_path\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)  # Crear subcarpetas si no existen\n",
    "    \n",
    "    with out_file.open(\"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "    print(\"✔ Cropped →\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70b69b",
   "metadata": {},
   "source": [
    "# NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a36ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS = [\n",
    "    r'^Superior Tribunal.*$',\n",
    "    r'^Sala Civil y Comercial.*$',\n",
    "    r'^\\s*\\d+\\s*$',\n",
    "    r'^Poder Judicial.*$',\n",
    "    r'^Firmado digitalmente.*$',\n",
    "    r'^Página \\s*\\d+(\\s*de\\s*\\d+)?',\n",
    "    r'^\\s*\\d{4}-\\d{2}-\\d{2}T\\d{2}:',\n",
    "]\n",
    "REGEX = re.compile('|'.join(PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def join_lines_to_paragraphs(text):\n",
    "    lines = text.split('\\n')\n",
    "    paragraphs = []\n",
    "    current_para = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            # Línea vacía indica fin de párrafo\n",
    "            if current_para:\n",
    "                paragraphs.append(current_para.strip())\n",
    "                current_para = \"\"\n",
    "        else:\n",
    "            # Si la línea anterior no termina con punto pero la linea actual no es solamente mayuscula y termina con ':', unir\n",
    "            if current_para and not current_para.endswith(('.', ':', '?', '!', ';')):\n",
    "                if line.isupper() and line.endswith(':') and len(line) < 50:\n",
    "                    # Si la línea es mayúscula y termina con ':', no unir\n",
    "                    paragraphs.append(current_para.strip())\n",
    "                    current_para = line\n",
    "                else:\n",
    "                    current_para += \" \" + line\n",
    "            else:\n",
    "                if current_para:\n",
    "                    paragraphs.append(current_para.strip())\n",
    "                current_para = line\n",
    "    if current_para:\n",
    "        paragraphs.append(current_para.strip())\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "def clean_page(text: str) -> str:\n",
    "    return '\\n'.join([ln for ln in (text or \"\").splitlines() if not REGEX.match(ln)]).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c3b5",
   "metadata": {},
   "source": [
    "## Normalization to .TXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13cc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning PDFs: 100%|██████████| 10/10 [00:02<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def clean_pdf(pdf_path: Path) -> str:\n",
    "    pages_clean = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = clean_page(page.extract_text())\n",
    "            page_text = join_lines_to_paragraphs(page_text)  # <--- aplicar aquí\n",
    "            pages_clean.append(page_text)\n",
    "    return \"\\n\\n\".join(pages_clean)\n",
    "\n",
    "\n",
    "def main(pdf_dir: str, out_dir: str):\n",
    "    pdf_root = Path(pdf_dir).resolve()\n",
    "    out_root = Path(out_dir).resolve()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_root.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No se encontraron PDFs en {pdf_dir}\")\n",
    "        return\n",
    "\n",
    "    for path in tqdm(pdf_files, desc=\"Cleaning PDFs\"):\n",
    "        # —— NUEVO: ruta relativa para reproducir subcarpetas ——\n",
    "        rel_path = path.relative_to(pdf_root).with_suffix(\".txt\")\n",
    "        dst = out_root / rel_path\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)  # crea la subcarpeta si falta\n",
    "        # --------------------------------------------------------\n",
    "        dst.write_text(clean_pdf(path), encoding=\"utf-8\")\n",
    "\n",
    "main(PATH_RECORTES, PATH_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b92c9",
   "metadata": {},
   "source": [
    "## Normalization to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b75648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning PDFs: 100%|██████████| 10/10 [00:02<00:00,  4.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_materia_preliminar(inicio_paragraphs):\n",
    "    \"\"\"\n",
    "    Busca dentro de los párrafos de INICIO el texto entre comillas,\n",
    "    y extrae el texto que sigue justo después de 'S/' dentro de esa cadena.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    for para in inicio_paragraphs:\n",
    "        # Buscamos todas las cadenas entre comillas\n",
    "        quoted_texts = re.findall(r'\"([^\"]+)\"', para)\n",
    "        for qt in quoted_texts:\n",
    "            # Buscamos 'S/' y capturamos lo que viene después hasta el fin o hasta un guion, coma o fin de línea\n",
    "            m = re.search(r'S/\\s*([^\\-\\,]+)', qt, re.IGNORECASE)\n",
    "            if m:\n",
    "                materia = m.group(1).strip()\n",
    "                return materia.upper()\n",
    "    return None\n",
    "\n",
    "def is_key_line(line: str) -> bool:\n",
    "    # Es key si toda mayúscula, termina con ':' y longitud < 50 (para evitar líneas muy largas)\n",
    "    return line.isupper() and line.endswith(':') and len(line) < 50\n",
    "\n",
    "def split_into_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Convierte el texto en dict con keys y listas de párrafos.\n",
    "    La key 'INICIO' contiene todo lo previo a la primera key.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n\\n')  # separar párrafos\n",
    "    sections = {}\n",
    "    current_key = 'INICIO'\n",
    "    sections[current_key] = []\n",
    "\n",
    "    for para in lines:\n",
    "        para_strip = para.strip()\n",
    "        if is_key_line(para_strip):\n",
    "            current_key = para_strip[:-1]  # sacamos ':'\n",
    "            if current_key not in sections:\n",
    "                sections[current_key] = []\n",
    "        else:\n",
    "            sections[current_key].append(para_strip)\n",
    "    return sections\n",
    "\n",
    "\n",
    "\n",
    "def clean_pdf_no_join(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto limpio de todo el PDF, concatenando páginas\n",
    "    sin unir líneas en párrafos todavía para evitar cortar párrafos.\n",
    "    \"\"\"\n",
    "    pages_text = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = clean_page(page.extract_text())\n",
    "            pages_text.append(page_text.strip())\n",
    "    # Unir páginas con salto de línea simple para no cortar párrafos\n",
    "    return \"\\n\".join(pages_text)\n",
    "\n",
    "def clean_pdf_to_sections_structured(pdf_path: Path) -> dict:\n",
    "    full_text = clean_pdf_no_join(pdf_path)\n",
    "    full_text = join_lines_to_paragraphs(full_text)\n",
    "    sections = split_into_sections(full_text)\n",
    "    materia = extract_materia_preliminar(sections.get('INICIO', []))\n",
    "    \n",
    "    return {\n",
    "        \"INFORMACION\": {\n",
    "            \"MATERIA_PRELIMINAR\": materia or \"\",\n",
    "            \"RESUMEN\": \"\"\n",
    "        },\n",
    "        \"CONTENIDO\": sections\n",
    "    }\n",
    "\n",
    "\n",
    "def main(pdf_dir: str, out_dir: str):\n",
    "    pdf_root = Path(pdf_dir).resolve()\n",
    "    out_root = Path(out_dir).resolve()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_root.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No se encontraron PDFs en {pdf_dir}\")\n",
    "        return\n",
    "\n",
    "    for path in tqdm(pdf_files, desc=\"Cleaning PDFs\"):\n",
    "        rel_path = path.relative_to(pdf_root).with_suffix(\".json\")\n",
    "        dst = out_root / rel_path\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        sections_structured = clean_pdf_to_sections_structured(path)\n",
    "        dst.write_text(json.dumps([sections_structured], ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "main(PATH_RECORTES, PATH_JSON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
