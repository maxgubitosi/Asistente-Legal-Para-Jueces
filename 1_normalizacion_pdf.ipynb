{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74237701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re, os, argparse, pdfplumber\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "for name in (\"pdfminer\", \"pdfminer.layout\", \"pdfminer.pdfpage\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29fcaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_GLOBAL = os.getcwd()\n",
    "PATH = os.path.join(PATH_GLOBAL, \"datasets\")\n",
    "\n",
    "\n",
    "# Prueba inicial con solo un mes (febrero 2024 - 10 fallos)\n",
    "PATH_FALLOS = Path(os.path.join(PATH, \"fallos\"))\n",
    "PATH_RECORTES = Path(os.path.join(PATH, \"cropped_pdfs\"))\n",
    "PATH_TXT = Path(os.path.join(PATH, \"fallos_txts\"))\n",
    "PATH_JSON = Path(os.path.join(PATH, \"fallos_json\"))\n",
    "\n",
    "os.makedirs(PATH_RECORTES, exist_ok=True)\n",
    "os.makedirs(PATH_TXT, exist_ok=True)\n",
    "os.makedirs(PATH_JSON, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10af937",
   "metadata": {},
   "source": [
    "## Recorte de PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecba0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8104.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8142.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8344.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8569.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8752.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8865.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8926.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8927.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8948.pdf\n",
      "✔ Cropped → c:\\Users\\MSI\\Desktop\\Udesa\\Materias\\2025 - 1er Semestre\\NLP\\Proyecto Final\\TP_NLP\\datasets\\cropped_pdfs\\2024\\02\\8971.pdf\n"
     ]
    }
   ],
   "source": [
    "HEADER_PT = 95      # quitar SOLO en páginas impares (arriba)\n",
    "FOOTER_PT = 50      # quitar SIEMPRE (abajo)\n",
    "\n",
    "for pdf in PATH_FALLOS.rglob(\"*.pdf\"):\n",
    "    reader, writer = PdfReader(str(pdf)), PdfWriter()\n",
    "    for idx, page in enumerate(reader.pages, start=1):\n",
    "        box = page.cropbox                       # caja de recorte base\n",
    "        # -- recorte inferior --\n",
    "        box.lower_left  = (box.lower_left[0],  box.lower_left[1] + FOOTER_PT)\n",
    "        # -- recorte superior en páginas impares --\n",
    "        if idx % 2 == 1:\n",
    "            box.upper_right = (box.upper_right[0], box.upper_right[1] - HEADER_PT)\n",
    "\n",
    "        # aplicar a todos los bounding-boxes que respetan los visores/lectores\n",
    "        page.cropbox  = box\n",
    "        page.trimbox  = box\n",
    "        page.mediabox = box\n",
    "        writer.add_page(page)\n",
    "\n",
    "    # Mantener la estructura de subcarpetas en el output\n",
    "    rel_path = pdf.relative_to(PATH_FALLOS)\n",
    "    out_file = PATH_RECORTES / rel_path\n",
    "    out_file.parent.mkdir(parents=True, exist_ok=True)  # Crear subcarpetas si no existen\n",
    "    \n",
    "    with out_file.open(\"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "    print(\"✔ Cropped →\", out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70b69b",
   "metadata": {},
   "source": [
    "# NORMALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a36ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERNS = [\n",
    "    r'^Superior Tribunal.*$',\n",
    "    r'^Sala Civil y Comercial.*$',\n",
    "    r'^\\s*\\d+\\s*$',\n",
    "    r'^Poder Judicial.*$',\n",
    "    r'^Firmado digitalmente.*$',\n",
    "    r'^Página \\s*\\d+(\\s*de\\s*\\d+)?',\n",
    "    r'^\\s*\\d{4}-\\d{2}-\\d{2}T\\d{2}:',\n",
    "]\n",
    "REGEX = re.compile('|'.join(PATTERNS), re.IGNORECASE)\n",
    "\n",
    "\n",
    "def join_lines_to_paragraphs(text):\n",
    "    lines = text.split('\\n')\n",
    "    paragraphs = []\n",
    "    current_para = \"\"\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            # Línea vacía indica fin de párrafo\n",
    "            if current_para:\n",
    "                paragraphs.append(current_para.strip())\n",
    "                current_para = \"\"\n",
    "        else:\n",
    "            # Si la línea anterior no termina con punto pero la linea actual no es solamente mayuscula y termina con ':', unir\n",
    "            if current_para and not current_para.endswith(('.', ':', '?', '!', ';')):\n",
    "                if line.isupper() and line.endswith(':') and len(line) < 50:\n",
    "                    # Si la línea es mayúscula y termina con ':', no unir\n",
    "                    paragraphs.append(current_para.strip())\n",
    "                    current_para = line\n",
    "                else:\n",
    "                    current_para += \" \" + line\n",
    "            else:\n",
    "                if current_para:\n",
    "                    paragraphs.append(current_para.strip())\n",
    "                current_para = line\n",
    "    if current_para:\n",
    "        paragraphs.append(current_para.strip())\n",
    "    return \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "def clean_page(text: str) -> str:\n",
    "    return '\\n'.join([ln for ln in (text or \"\").splitlines() if not REGEX.match(ln)]).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345c3b5",
   "metadata": {},
   "source": [
    "## Normalization to .TXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13cc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning PDFs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning PDFs: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "def clean_pdf(pdf_path: Path) -> str:\n",
    "    pages_clean = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = clean_page(page.extract_text())\n",
    "            page_text = join_lines_to_paragraphs(page_text)  # <--- aplicar aquí\n",
    "            pages_clean.append(page_text)\n",
    "    return \"\\n\\n\".join(pages_clean)\n",
    "\n",
    "\n",
    "def main(pdf_dir: str, out_dir: str):\n",
    "    pdf_root = Path(pdf_dir).resolve()\n",
    "    out_root = Path(out_dir).resolve()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_root.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No se encontraron PDFs en {pdf_dir}\")\n",
    "        return\n",
    "\n",
    "    for path in tqdm(pdf_files, desc=\"Cleaning PDFs\"):\n",
    "        # —— NUEVO: ruta relativa para reproducir subcarpetas ——\n",
    "        rel_path = path.relative_to(pdf_root).with_suffix(\".txt\")\n",
    "        dst = out_root / rel_path\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)  # crea la subcarpeta si falta\n",
    "        # --------------------------------------------------------\n",
    "        dst.write_text(clean_pdf(path), encoding=\"utf-8\")\n",
    "\n",
    "main(PATH_RECORTES, PATH_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b92c9",
   "metadata": {},
   "source": [
    "## Normalization to JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b75648a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PDF to JSON:: 100%|██████████| 297/297 [01:11<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_materia_preliminar(inicio_paragraphs):\n",
    "    \"\"\"\n",
    "    Busca dentro de los párrafos de INICIO el texto entre comillas,\n",
    "    y extrae el texto que sigue justo después de 'S/' dentro de esa cadena.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    for para in inicio_paragraphs:\n",
    "        # Buscamos todas las cadenas entre comillas\n",
    "        quoted_texts = re.findall(r'\"([^\"]+)\"', para)\n",
    "        for qt in quoted_texts:\n",
    "            # Buscamos 'S/' y capturamos lo que viene después hasta el fin o hasta un guion, coma o fin de línea\n",
    "            m = re.search(r'S/\\s*([^\\-\\,]+)', qt, re.IGNORECASE)\n",
    "            if m:\n",
    "                materia = m.group(1).strip()\n",
    "                return materia.upper()\n",
    "    return None\n",
    "\n",
    "def is_key_line(line: str) -> bool:\n",
    "    # Es key si toda mayúscula, termina con ':' y longitud < 50 (para evitar líneas muy largas)\n",
    "    return line.isupper() and line.endswith(':') and len(line) < 50\n",
    "\n",
    "def split_into_sections(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Convierte el texto en dict con keys y listas de párrafos.\n",
    "    La key 'INICIO' contiene todo lo previo a la primera key.\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n\\n')  # separar párrafos\n",
    "    sections = {}\n",
    "    current_key = 'INICIO'\n",
    "    sections[current_key] = []\n",
    "\n",
    "    for para in lines:\n",
    "        para_strip = para.strip()\n",
    "        if is_key_line(para_strip):\n",
    "            current_key = para_strip[:-1]  # sacamos ':'\n",
    "            if current_key not in sections:\n",
    "                sections[current_key] = []\n",
    "        else:\n",
    "            sections[current_key].append(para_strip)\n",
    "    return sections\n",
    "\n",
    "\n",
    "\n",
    "def clean_pdf_no_join(pdf_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Extrae el texto limpio de todo el PDF, concatenando páginas\n",
    "    sin unir líneas en párrafos todavía para evitar cortar párrafos.\n",
    "    \"\"\"\n",
    "    pages_text = []\n",
    "    with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = clean_page(page.extract_text())\n",
    "            pages_text.append(page_text.strip())\n",
    "    # Unir páginas con salto de línea simple para no cortar párrafos\n",
    "    return \"\\n\".join(pages_text)\n",
    "\n",
    "def clean_pdf_to_sections_structured(pdf_path: Path) -> dict:\n",
    "    full_text = clean_pdf_no_join(pdf_path)\n",
    "    full_text = join_lines_to_paragraphs(full_text)\n",
    "    sections = split_into_sections(full_text)\n",
    "    materia = extract_materia_preliminar(sections.get('INICIO', []))\n",
    "    \n",
    "    return {\n",
    "        \"INFORMACION\": {\n",
    "            \"MATERIA_PRELIMINAR\": materia or \"\",\n",
    "            \"RESUMEN\": \"\"\n",
    "        },\n",
    "        \"CONTENIDO\": sections\n",
    "    }\n",
    "\n",
    "\n",
    "def main(pdf_dir: str, out_dir: str):\n",
    "    pdf_root = Path(pdf_dir).resolve()\n",
    "    out_root = Path(out_dir).resolve()\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pdf_files = list(pdf_root.rglob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        print(f\"No se encontraron PDFs en {pdf_dir}\")\n",
    "        return\n",
    "\n",
    "    for path in tqdm(pdf_files, desc=\"PDF to JSON:\"):\n",
    "        rel_path = path.relative_to(pdf_root).with_suffix(\".json\")\n",
    "        dst = out_root / rel_path\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        sections_structured = clean_pdf_to_sections_structured(path)\n",
    "        dst.write_text(json.dumps([sections_structured], ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "main(PATH_RECORTES, PATH_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e853617",
   "metadata": {},
   "source": [
    "## JSON Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279859c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ANÁLISIS DE ESTRUCTURA - 297 archivos JSON\n",
      "============================================================\n",
      "🗂️  ARCHIVOS PROCESADOS: 297\n",
      "📋 ARCHIVOS CON MATERIA: 273\n",
      "\n",
      "🔧 KEYS EN 'INFORMACION':\n",
      "  • MATERIA_PRELIMINAR: 297 archivos (100.0%)\n",
      "  • RESUMEN: 297 archivos (100.0%)\n",
      "\n",
      "📖 SECCIONES EN 'CONTENIDO' (Top 20):\n",
      "  • INICIO: 297 archivos (100.0%)\n",
      "  • RESUELVE: 294 archivos (99.0%)\n",
      "  • Y VISTO: 145 archivos (48.8%)\n",
      "  • ACUERDO: 134 archivos (45.1%)\n",
      "  • CONSIDERANDO: 85 archivos (28.6%)\n",
      "  • VISTO: 84 archivos (28.3%)\n",
      "  • VISTO Y CONSIDERANDO: 66 archivos (22.2%)\n",
      "  • FEDERICO TEPSICH DIJO: 42 archivos (14.1%)\n",
      "  • VOCAL GISELA N. SCHUMACHER DIJO: 37 archivos (12.5%)\n",
      "  • GISELA N. SCHUMACHER DIJO: 34 archivos (11.4%)\n",
      "  • CARLOS FEDERICO TEPSICH DIJO: 34 archivos (11.4%)\n",
      "  • HONORARIOS PROFESIONALES: 28 archivos (9.4%)\n",
      "  • LEONARDO PORTELA DIJO: 26 archivos (8.8%)\n",
      "  • N. SCHUMACHER DIJO: 16 archivos (5.4%)\n",
      "  • SCHUMACHER DIJO: 14 archivos (4.7%)\n",
      "  • DIJERON: 14 archivos (4.7%)\n",
      "  • VOCAL GISELA SCHUMACHER DIJO: 14 archivos (4.7%)\n",
      "  • SCHUMACHER DIJERON: 13 archivos (4.4%)\n",
      "  • ///CUERDO: 11 archivos (3.7%)\n",
      "  • GISELA SCHUMACHER DIJO: 10 archivos (3.4%)\n",
      "  ... y 39 secciones más\n",
      "\n",
      "📊 ESTADÍSTICAS DE SECCIONES POR ARCHIVO:\n",
      "  • Promedio: 5.0 secciones\n",
      "  • Mínimo: 2 secciones\n",
      "  • Máximo: 10 secciones\n",
      "\n",
      "📋 TOTAL DE SECCIONES ÚNICAS ENCONTRADAS: 59\n"
     ]
    }
   ],
   "source": [
    "def analyze_json_structures(json_dir: str):\n",
    "    \"\"\"\n",
    "    Analiza la estructura de todos los JSONs extraídos:\n",
    "    - Cuenta total de archivos\n",
    "    - Lista todas las keys encontradas\n",
    "    - Cuenta frecuencia de cada key\n",
    "    - Muestra estadísticas de estructura\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from collections import Counter, defaultdict\n",
    "    from pathlib import Path\n",
    "    \n",
    "    json_root = Path(json_dir).resolve()\n",
    "    json_files = list(json_root.rglob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No se encontraron JSONs en {json_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📊 ANÁLISIS DE ESTRUCTURA - {len(json_files)} archivos JSON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Contadores para análisis\n",
    "    contenido_keys = Counter()\n",
    "    informacion_keys = Counter()\n",
    "    total_sections_per_file = []\n",
    "    files_with_materia = 0\n",
    "    \n",
    "    for json_path in json_files:\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)[0]  # Primer elemento de la lista\n",
    "            \n",
    "            # Analizar INFORMACION\n",
    "            if 'INFORMACION' in data:\n",
    "                for key in data['INFORMACION'].keys():\n",
    "                    informacion_keys[key] += 1\n",
    "                \n",
    "                # Contar archivos con materia\n",
    "                materia = data['INFORMACION'].get('MATERIA_PRELIMINAR', '')\n",
    "                if materia and materia.strip():\n",
    "                    files_with_materia += 1\n",
    "            \n",
    "            # Analizar CONTENIDO\n",
    "            if 'CONTENIDO' in data:\n",
    "                sections = data['CONTENIDO']\n",
    "                total_sections_per_file.append(len(sections))\n",
    "                \n",
    "                for key in sections.keys():\n",
    "                    contenido_keys[key] += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error procesando {json_path}: {e}\")\n",
    "    \n",
    "    # RESULTADOS\n",
    "    print(f\"🗂️  ARCHIVOS PROCESADOS: {len(json_files)}\")\n",
    "    print(f\"📋 ARCHIVOS CON MATERIA: {files_with_materia}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔧 KEYS EN 'INFORMACION':\")\n",
    "    for key, count in informacion_keys.most_common():\n",
    "        percentage = (count / len(json_files)) * 100\n",
    "        print(f\"  • {key}: {count} archivos ({percentage:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📖 SECCIONES EN 'CONTENIDO' (Top 20):\")\n",
    "    for key, count in contenido_keys.most_common(20):\n",
    "        percentage = (count / len(json_files)) * 100\n",
    "        print(f\"  • {key}: {count} archivos ({percentage:.1f}%)\")\n",
    "    \n",
    "    if len(contenido_keys) > 20:\n",
    "        print(f\"  ... y {len(contenido_keys) - 20} secciones más\")\n",
    "    print()\n",
    "    \n",
    "    # Estadísticas de secciones por archivo\n",
    "    if total_sections_per_file:\n",
    "        avg_sections = sum(total_sections_per_file) / len(total_sections_per_file)\n",
    "        min_sections = min(total_sections_per_file)\n",
    "        max_sections = max(total_sections_per_file)\n",
    "        \n",
    "        print(\"📊 ESTADÍSTICAS DE SECCIONES POR ARCHIVO:\")\n",
    "        print(f\"  • Promedio: {avg_sections:.1f} secciones\")\n",
    "        print(f\"  • Mínimo: {min_sections} secciones\")\n",
    "        print(f\"  • Máximo: {max_sections} secciones\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"📋 TOTAL DE SECCIONES ÚNICAS ENCONTRADAS: {len(contenido_keys)}\")\n",
    "    \n",
    "    return {\n",
    "        'total_files': len(json_files),\n",
    "        'files_with_materia': files_with_materia,\n",
    "        'contenido_keys': dict(contenido_keys),\n",
    "        'informacion_keys': dict(informacion_keys),\n",
    "        'sections_stats': {\n",
    "            'avg': avg_sections if total_sections_per_file else 0,\n",
    "            'min': min_sections if total_sections_per_file else 0,\n",
    "            'max': max_sections if total_sections_per_file else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Ejecutar análisis\n",
    "analysis_results = analyze_json_structures(PATH_JSON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
